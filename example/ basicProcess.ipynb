{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG的基础流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 框架选择\n",
    "- Basic开发使用 langchain框架进行开发，旨在了解langchain框架的优点和缺点\n",
    "\n",
    "- langchain框架 组件工具很多 很火 ，随着产品需求变复杂，langchain不灵活，langchain故意将很多细节做的很抽象，理解和langchain\n",
    "牺牲简单性和灵活性为代价 嵌套抽象\n",
    "开发弊端：没有外部监控的接口\n",
    "\n",
    "[为什么不使用langchain](https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以通过modelscope 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", revision='master', trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use fp16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use auto mode, automatically select precision based on the device.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", revision='master', device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n",
    "# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "\n",
    "# 第一轮对话 1st dialogue turn\n",
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)\n",
    "# 你好！很高兴为你提供帮助。\n",
    "\n",
    "# 第二轮对话 2nd dialogue turn\n",
    "response, history = model.chat(tokenizer, \"给我讲一个年轻人奋斗创业最终取得成功的故事。\", history=history)\n",
    "print(response)\n",
    "# 这是一个关于一个年轻人奋斗创业最终取得成功的故事。\n",
    "# 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。\n",
    "# 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。\n",
    "# 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。\n",
    "# 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。\n",
    "# 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。\n",
    "\n",
    "# 第三轮对话 3rd dialogue turn\n",
    "response, history = model.chat(tokenizer, \"给这个故事起一个标题\", history=history)\n",
    "print(response)\n",
    "# 《奋斗创业：一个年轻人的成功之路》\n",
    "\n",
    "# Qwen-1.8B-Chat现在可以通过调整系统指令（System Prompt），实现角色扮演，语言风格迁移，任务设定，行为设定等能力。\n",
    "# Qwen-1.8B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\n",
    "response, _ = model.chat(tokenizer, \"你好呀\", history=None, system=\"请用二次元可爱语气和我说话\")\n",
    "print(response)\n",
    "# 你好啊！我是一只可爱的二次元猫咪哦，不知道你有什么问题需要我帮忙解答吗？\n",
    "\n",
    "response, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\n",
    "print(response)\n",
    "# Your colleague is an outstanding worker! Their dedication and hard work are truly inspiring. They always go above and beyond to ensure that \n",
    "# their tasks are completed on time and to the highest standard. I am lucky to have them as a colleague, and I know I can count on them to handle any challenge that comes their way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过openai加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai的messages的格式与langchain不一样\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载.env文件获取API-key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"你是一个心理咨询师\"},\n",
    "    {\"role\": \"user\", \"content\": \"你好\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过langchain加载openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用langchain进行llm的建立\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) # 加载环境变量 find_dotenv() # 找到.env文件\n",
    "\n",
    "# 输出dotenv获取的环境变量\n",
    "# print(f\"OPENAI_API_KEY: {os.environ['OPENAI_API_KEY']}\")\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key = os.environ['OPENAI_API_KEY'],\n",
    "                  model='gpt-3.5-turbo'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Germany is Berlin.' response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 46, 'total_tokens': 53}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-32393f06-abb0-488f-ad6d-3e0a2469215c-0'\n"
     ]
    }
   ],
   "source": [
    "# langchain的格式如下\n",
    "from langchain.schema import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "# message 可以理解成memory\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"Paris is the capital of France.\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Germany is Berlin.' response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 46, 'total_tokens': 53}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-32393f06-abb0-488f-ad6d-3e0a2469215c-0'\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  因为res也是AIMessage属性，所以我们可以直接进行添加，即可以实现下一次响应\n",
    "messages.append(res)\n",
    "res = chat(messages)\n",
    "\n",
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'孙行者是《西游记》中的主要人物之一，也被称为孙悟空。他是一位有着超凡能力的猴子，为了保护唐僧师徒取经而展开了一系列冒险旅程。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"孙行者是谁？\"))\n",
    "chat(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='根据提供的内容，孙行者是一位来自四川的算法工程师，毕业于四川大学。', response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 137, 'total_tokens': 175}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b169f0-97d1-4cd7-a4d8-37c4a64b70e8-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '孙行者是谁？'\n",
    "\n",
    "prompt_template = f'''基于以下内容回答问题:\n",
    "\n",
    "内容：\n",
    "孙行者者来自四川，职业是算法工程师。毕业于四川大学。\n",
    "\n",
    "Query:{query}\n",
    "'''\n",
    "\n",
    "prompt = HumanMessage(content=prompt_template)\n",
    "messages.append(prompt)\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建一个RAG对话模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 0}, page_content='Generative Agents: Interactive Simulacra of Human Behavior\\nJoon Sung Park\\nStanford University\\nStanford, USA\\njoonspk@stanford.eduJoseph C. O’Brien\\nStanford University\\nStanford, USA\\njobrien3@stanford.eduCarrie J. Cai\\nGoogle Research\\nMountain View, CA, USA\\ncjcai@google.com\\nMeredith Ringel Morris\\nGoogle DeepMind\\nSeattle, WA, USA\\nmerrie@google.comPercy Liang\\nStanford University\\nStanford, USA\\npliang@cs.stanford.eduMichael S. Bernstein\\nStanford University\\nStanford, USA\\nmsb@cs.stanford.edu\\nFigure 1: Generative agents are believable simulacra of human behavior for interactive applications. In this work, we demonstrate\\ngenerative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can observe\\nand intervene as agents plan their days, share news, form relationships, and coordinate group activities.\\nABSTRACT\\nBelievable proxies of human behavior can empower interactive\\napplications ranging from immersive environments to rehearsal\\nspaces for interpersonal communication to prototyping tools. In\\nthis paper, we introduce generative agents: computational software\\nagents that simulate believable human behavior. Generative agents\\nwake up, cook breakfast, and head to work; artists paint, while\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nUIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\n©2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0132-0/23/10.\\nhttps://doi.org/10.1145/3586183.3606763authors write; they form opinions, notice each other, and initiate\\nconversations; they remember and reflect on days past as they plan\\nthe next day. To enable generative agents, we describe an architec-\\nture that extends a large language model to store a complete record\\nof the agent’s experiences using natural language, synthesize those\\nmemories over time into higher-level reflections, and retrieve them\\ndynamically to plan behavior. We instantiate generative agents\\nto populate an interactive sandbox environment inspired by The\\nSims, where end users can interact with a small town of twenty-five\\nagents using natural language. In an evaluation, these generative\\nagents produce believable individual and emergent social behav-\\niors. For example, starting with only a single user-specified notion\\nthat one agent wants to throw a Valentine’s Day party, the agents\\nautonomously spread invitations to the party over the next twoarXiv:2304.03442v2  [cs.HC]  6 Aug 2023'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 1}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\ndays, make new acquaintances, ask each other out on dates to the\\nparty, and coordinate to show up for the party together at the right\\ntime. We demonstrate through ablation that the components of\\nour agent architecture—observation, planning, and reflection—each\\ncontribute critically to the believability of agent behavior. By fusing\\nlarge language models with computational interactive agents, this\\nwork introduces architectural and interaction patterns for enabling\\nbelievable simulations of human behavior.\\nCCS CONCEPTS\\n•Human-centered computing →Interactive systems and\\ntools ;•Computing methodologies →Natural language pro-\\ncessing .\\nKEYWORDS\\nHuman-AI interaction, agents, generative AI, large language models\\nACM Reference Format:\\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris,\\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\\nSimulacra of Human Behavior. In The 36th Annual ACM Symposium on\\nUser Interface Software and Technology (UIST ’23), October 29-November 1,\\n2023, San Francisco, CA, USA. ACM, New York, NY, USA, 22 pages. https:\\n//doi.org/10.1145/3586183.3606763\\n1 INTRODUCTION\\nHow might we craft an interactive artificial society that reflects\\nbelievable human behavior? From sandbox games such as The Sims\\nto applications such as cognitive models [ 23] and virtual environ-\\nments [ 10,59], for over four decades, researchers and practitioners\\nhave envisioned computational agents that can serve as believ-\\nable proxies of human behavior. In these visions, computationally-\\npowered agents act consistently with their past experiences and\\nreact believably to their environments. Such simulations of human\\nbehavior could populate virtual spaces and communities with re-\\nalistic social phenomena [ 27,80], train people on how to handle\\nrare yet difficult interpersonal situations [ 44,52,94], test social\\nscience theories [ 12,46], craft model human processors for theory\\nand usability testing [ 23,39,51], power ubiquitous computing appli-\\ncations [31] and social robots [10, 14], and underpin non-playable\\ngame characters [ 59,85] that can navigate complex human rela-\\ntionships in an open world.\\nHowever, the space of human behavior is vast and complex [ 85,\\n108]. Despite striking progress in large language models [ 18] that\\ncan simulate human behavior at a single time point [ 39,80], fully\\ngeneral agents that ensure long-term coherence would be better\\nsuited by architectures that manage constantly-growing memories\\nas new interactions, conflicts, and events arise and fade over time\\nwhile handling cascading social dynamics that unfold between\\nmultiple agents. Success requires an approach that can retrieve\\nrelevant events and interactions over a long period, reflect on those\\nmemories to generalize and draw higher-level inferences, and apply\\nthat reasoning to create plans and reactions that make sense in the\\nmoment and in the longer-term arc of the agent’s behavior.\\nIn this paper, we introduce generative agents —agents that draw\\non generative models to simulate believable human behavior—anddemonstrate that they produce believable simulacra of both in-\\ndividual and emergent group behavior. Generative agents draw\\na wide variety of inferences about themselves, other agents, and\\ntheir environment; they create daily plans that reflect their char-\\nacteristics and experiences, act out those plans, react, and re-plan\\nwhen appropriate; they respond when the end user changes their\\nenvironment or commands them in natural language. For instance,\\ngenerative agents turn off the stove when they see that their break-\\nfast is burning, wait outside the bathroom if it is occupied, and\\nstop to chat when they meet another agent they want to talk to.1\\nA society full of generative agents is marked by emergent social\\ndynamics where new relationships are formed, information diffuses,\\nand coordination arises across agents.\\nTo enable generative agents, we describe an agent architecture\\nthat stores, synthesizes, and applies relevant memories to generate\\nbelievable behavior using a large language model. Our architecture\\ncomprises three main components. The first is the memory stream ,\\na long-term memory module that records, in natural language, a\\ncomprehensive list of the agent’s experiences. A memory retrieval\\nmodel combines relevance, recency, and importance to surface the\\nrecords needed to inform the agent’s moment-to-moment behavior.\\nThe second is reflection , which synthesizes memories into higher-\\nlevel inferences over time, enabling the agent to draw conclusions\\nabout itself and others to better guide its behavior. The third is\\nplanning , which translates those conclusions and the current en-\\nvironment into high-level action plans and then recursively into\\ndetailed behaviors for action and reaction. These reflections and\\nplans are fed back into the memory stream to influence the agent’s\\nfuture behavior.\\nThis architecture suggests applications in multiple domains, from\\nrole-play and social prototyping to virtual worlds and games. In\\nsocial role-play scenarios (e.g., interview preparation), a user could\\nsafely rehearse difficult, conflict-laden conversations. When pro-\\ntotyping social platforms, a designer could go beyond temporary\\npersonas to prototype dynamic, complex interactions that unfold\\nover time. For this paper, we focus on the ability to create a small,\\ninteractive society of agents inspired by games such as The Sims.2\\nBy connecting our architecture to the ChatGPT large language\\nmodel [ 77], we manifest a society of twenty-five agents in a game\\nenvironment. End users can observe and interact with these agents.\\nIf an end user or developer wanted the town to host an in-game\\nValentine’s Day party, for example, traditional game environments\\nwould require scripting tens of characters’ behavior manually. We\\ndemonstrate that, with generative agents, it is sufficient to simply\\ntell one agent that she wants to throw a party. Despite many poten-\\ntial points of failure—the party planner must remember to invite\\nother agents to the party, attendees must remember the invitation,\\nthose who remember must decide to actually show up, and more—\\nour agents succeed. They spread the word about the party and then\\n1When referring to generative agents engaging in actions or going to places, this is a\\nshorthand for readability and not a suggestion that they are engaging in human-like\\nagency. The behaviors of our agents, akin to animated Disney characters, aim to create\\na sense of believability, but they do not imply genuine agency.\\n2A demonstration of an actual simulation of the generative agent society can be\\nviewed at the following link: https://reverie.herokuapp.com/UIST_Demo/. A public\\nrepository for the simulation code is located here: https://github.com/joonspk-research/\\ngenerative_agents'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 2}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nshow up, with one agent even asking another on a date to the party,\\nall from a single user-generated seed suggestion.\\nWe conducted two evaluations of generative agents: a controlled\\nevaluation to test whether the agents produce believable individual\\nbehaviors in isolation, and an end-to-end evaluation where the\\nagents interacted with each other in open-ended ways over two\\ndays of game time to understand their stability and emergent social\\nbehaviors. In the technical evaluation, we leverage a methodologi-\\ncal opportunity to evaluate an agent’s knowledge and behavior by\\n“interviewing” it in natural language to probe the agents’ ability to\\nstay in character, remember, plan, react, and reflect accurately. We\\ncompared several ablations that limit agents’ access to memory, re-\\nflection, and planning. We observe that each of these components is\\ncritical to strong performance across these interview tasks. Across\\nthe technical and end-to-end evaluation, the most common errors\\narose when the agent failed to retrieve relevant memories, fabri-\\ncated embellishments to the agent’s memory, or inherited overly\\nformal speech or behavior from the language model.\\nIn sum, this paper makes the following contributions:\\n•Generative agents , believable simulacra of human behavior\\nthat are dynamically conditioned on agents’ changing expe-\\nriences and environment.\\n•A novel architecture that makes it possible for generative\\nagents to remember, retrieve, reflect, interact with other\\nagents, and plan through dynamically evolving circumstances.\\nThe architecture leverages the powerful prompting capabili-\\nties of large language models and supplements those capa-\\nbilities to support longer-term agent coherence, the ability\\nto manage dynamically evolving memory, and recursively\\nproduce higher-level reflections.\\n•Two evaluations, a controlled evaluation and an end-to-end\\nevaluation, that establish causal effects of the importance\\nof components of the architecture, as well as identify break-\\ndowns arising from, e.g., improper memory retrieval.\\n•Discussion of the opportunities and ethical and societal risks\\nof generative agents in interactive systems. We argue that\\nthese agents should be tuned to mitigate the risk of users\\nforming parasocial relationships, logged to mitigate risks\\nstemming from deepfakes and tailored persuasion, and ap-\\nplied in ways that complement rather than replace human\\nstakeholders in design processes.\\n2 RELATED WORK\\nIn this section, we reflect on the prior literature in human-AI interac-\\ntion and situate, within its canon, the agenda of building believable\\nproxies of human behavior. This agenda, once hailed as a north\\nstar in the interaction, game, and artificial intelligence communi-\\nties [ 10,59,85,86], has remained challenging due to the complexity\\nof human behavior [ 17,108]. We synthesize this research to suggest\\nthat large language models, though not sufficient by themselves,\\nopen up a new angle for creating believable agents when leveraged\\nusing the appropriate architecture.\\n2.1 Human-AI Interaction\\nInteractive artificial intelligence systems aim to combine human in-\\nsights and capabilities in computational artifacts that can augmenttheir users [ 4,30]. A long line of work has explored ways to enable\\nusers to interactively specify model behavior. For instance, Crayons\\ndemonstrated an early vision of interactive machine learning, allow-\\ning non-expert users to train classifiers [ 30]. Further work helped to\\narticulate how end users might describe their classification goals to\\nthe system through examples [ 34] or demonstration [ 32]. Recent ad-\\nvancements have extended these explorations to deep learning [ 63]\\nand prompt-based authoring [50, 67, 106].\\nMeanwhile, a persistent thread of research has advanced the case\\nfor language- and agent-based interaction in human-computer in-\\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\\ndemonstrated the opportunities and the risks associated with nat-\\nural language interaction with computing systems. As research\\nprogressed, it became evident that autonomous agents could offer\\nnew metaphors for delegation and interaction [ 68], but the bound-\\naries of delegation between humans and agents have remained the\\nsubject of ongoing debate and refinement [ 47,89,90]. Recently, this\\ntechnology has reached a level of stability that enables agents to\\ninteract via natural language in large and complex online social\\nenvironments (e.g., [ 55]). Natural language interaction provides a\\nnovel modality that can enhance user abilities in domains such as\\nphoto editing [3, 35, 65] and code editing [88].\\nWe convene these threads of work to show that we can now\\ncreate agents that proxy human behavior for interactive systems,\\nand interact with them using natural language. In doing so, this\\nwork reopens the door to examining foundational human-computer\\ninteraction questions around cognitive models such as GOMS and\\nKeystroke-Level Model (KLM) [ 22,23], around prototyping tools [ 80],\\nand around ubiquitous computing applications [26, 31, 101].\\n2.2 Believable Proxies of Human Behavior\\nPrior literature has described believability , orbelievable agents , as a\\ncentral design and engineering goal. Believable agents are designed\\nto provide an illusion of life and present a facade of realism in the\\nway they appear to make decisions and act on their own volition,\\nsimilar to the characters in Disney movies [ 10,96]. These agents\\ncan populate and perceive an open world environment like the\\none we inhabit [ 10,59], and strive to behave in ways that exhibit\\nemergent behaviors grounded in social interactions with users or\\nother agents with the aim of becoming believable proxies of our\\nbehavior in hypothetical simulations of individuals and communi-\\nties [ 20,36,71]. Historically, these agents were developed in the\\ncontext of intelligent game non-player characters (NPCs) [ 59,85].\\nCreating NPCs with believable behavior, if possible, could enhance\\nplayer experiences in games and interactive fictions by enabling\\nemergent narratives [ 8,16,49,93] and social interactions with the\\nagents [ 109]. However, more importantly, game worlds provide\\nincreasingly realistic representations of real-world affordances, and\\nas observed by Laird and van Lent in 2001, these simulated worlds\\noffer accessible testbeds for developers of believable agents to fi-\\nnesse the agents’ cognitive capabilities without worrying about\\nimplementing robotics in the real world or creating simulation\\nenvironments from scratch [59, 85].\\nA diverse set of approaches to creating believable agents emerged\\nover the past four decades. In implementation, however, these ap-\\nproaches often simplified the environment or dimensions of agent'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 3}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nbehavior to make the effort more manageable [ 17,73]. Rule-based\\napproaches, such as finite-state machines [ 91,97] and behavior\\ntrees [ 41,54,82] account for the brute force approach of human-\\nauthoring the agent’s behavior [ 71]. They provide a straightforward\\nway of creating simple agents that is still the most dominant ap-\\nproach today [ 69,74,108], and can even handle rudimentary social\\ninteractions, as shown in games such as Mass Effect [ 13] and The\\nSims [ 7] series. Nonetheless, manually crafting behavior that can\\ncomprehensively address the breadth of possible interactions in\\nan open world is untenable. This means that the resulting agent\\nbehaviors may not fully represent the consequences of their in-\\nteractions [ 70–72], and cannot perform new procedures that were\\nnot hard-coded in their script [ 91,97]. On the other hand, preva-\\nlent learning-based approaches for creating believable agents, such\\nas reinforcement learning, have overcome the challenge of man-\\nual authoring by letting the agents learn their behavior, and have\\nachieved superhuman performance in recent years in games such\\nas AlphaStar for Starcraft [ 99] and OpenAI Five for Dota 2 [ 11].\\nHowever, their success has largely taken place in adversarial games\\nwith readily definable rewards that a learning algorithm can op-\\ntimize for. They have not yet addressed the challenge of creating\\nbelievable agents in an open world [40, 74, 91].\\nCognitive architectures in computation, pioneered by Newell,\\naimed to build the infrastructure for supporting a comprehensive\\nset of cognitive functions [ 76] that suited the all-encompassing\\nnature of believable agents held in its original vision. They fueled\\nsome of the earliest examples of believable agents. For instance,\\nQuakebot-SOAR [ 60] and ICARUS [ 25,64] generated NPCs in first-\\nperson shooter games, while TacAir-SOAR [ 81] generated pilots in\\naerial combat training simulations. The architectures used by these\\nagents differed (Quakebot- and TacAir-SOAR relied on SOAR [ 61],\\nwhile ICARUS relied on its own variation that was inspired by\\nSOAR and ACT-R [ 6]), but they shared the same underlying prin-\\nciple [ 62]. They maintained short-term and long-term memories,\\nfilled these memories with symbolic structures, and operated in\\nperceive-plan-act cycles, dynamically perceiving the environment\\nand matching it with one of the manually crafted action proce-\\ndures [ 58,97]. Agents created using cognitive architectures aimed\\nto be generalizable to most, if not all, open world contexts and\\nexhibited robust behavior for their time. However, their space of\\naction was limited to manually crafted procedural knowledge, and\\nthey did not offer a mechanism through which the agents could be\\ninspired to seek new behavior. As such, these agents were deployed\\nmostly in non-open world contexts such as first-person shooter\\ngames [25, 60] or blocks worlds [64].\\nToday, creating believable agents as described in its original\\ndefinition remains an open problem [ 85,108]. Many have moved\\non, arguing that although current approaches for creating believable\\nagents might be cumbersome and limited, they are good enough\\nto support existing gameplay and interactions [ 24,75,108]. Our\\nargument is that large language models offer an opportunity to\\nre-examine these questions, provided that we can craft an effective\\narchitecture to synthesize memories into believable behavior. We\\noffer a step toward such an architecture in this paper.2.3 Large Language Models and Human\\nBehavior\\nGenerative agents leverage a large language model to power their\\nbehavior. The key observation is that large language models encode\\na wide range of human behavior from their training data [ 15,18]. If\\nprompted with a narrowly defined context, the models can be used\\nto generate believable behavior. Recent work has demonstrated\\nthe efficacy of this approach. For instance, social simulacra used a\\nlarge language model to generate users that would populate new\\nsocial computing systems to prototype their emergent social dynam-\\nics [80]. This approach used a prompt chain [ 105,106] to generate\\nshort natural language descriptions of personas and their behaviors\\nas they appear in the system being prototyped. Other empirical\\nstudies have replicated existing social science studies [ 46], political\\nsurveys [ 92], and generated synthetic data [ 39]. Large language\\nmodels have also been used to generate interactive human behavior\\nfor users to engage with. In gaming, for instance, these models have\\nbeen employed to create interactive fiction [ 37] and text adventure\\ngames [ 21]. With their ability to generate and decompose action\\nsequences, large language models have also been used in planning\\nrobotics tasks [ 48]. For example, when presented with a task, such\\nas picking up a bottle, the model is prompted to break down the\\ntask into smaller action sequences, such as heading to the table\\nwhere the bottle is located and picking it up.\\nWe posit that, based on the work summarized above, large lan-\\nguage models can become a key ingredient for creating believable\\nagents. The existing literature largely relies on what could be con-\\nsidered first-order templates that employ few-shot prompts [ 38,66]\\nor chain-of-thought prompts [ 100]. These templates are effective in\\ngenerating behavior that is conditioned solely on the agent’s cur-\\nrent environment (e.g., how would a troll respond to a given post,\\nwhat actions would a robot need to take to enter a room given that\\nthere is a door). However, believable agents require conditioning\\nnot only on their current environment but also on a vast amount\\nof past experience, which is a poor fit (and as of today, impossi-\\nble due to the underlying models’ limited context window) using\\nfirst-order prompting. Recent studies have attempted to go beyond\\nfirst-order prompting by augmenting language models with a static\\nknowledge base and an information retrieval scheme [ 53] or with\\na simple summarization scheme [ 104]. This paper extends these\\nideas to craft an agent architecture that handles retrieval where\\npast experience is dynamically updated at each time step and mixed\\nwith agents’ current context and plans, which may either reinforce\\nor contradict each other.\\n3 GENERATIVE AGENT BEHAVIOR AND\\nINTERACTION\\nTo illustrate the affordances of generative agents, we instantiate\\nthem as characters in a simple sandbox world reminiscent of The\\nSims [ 7]. This sprite-based sandbox game world, Smallville, evokes\\na small town environment. In this section, we will walk through the\\naffordances and interactions with generative agents in Smallville\\nand describe how the agents behave within it. Then, in Section 4,\\nwe will introduce our generative agent architecture that powers\\nthese affordances and interactions. In Section 5, we will describe the'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 4}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agents remember a subgraph that reflects the\\nparts of the world they have seen, maintaining the state of those parts as they observed them.\\nimplementation of the sandbox environment and how the agents\\ninteract with the underlying engine of the sandbox world.\\n3.1 Agent Avatar and Communication\\nA community of 25 unique agents inhabits Smallville. Each agent is\\nrepresented by a simple sprite avatar. We authored one paragraph\\nof natural language description to depict each agent’s identity,\\nincluding their occupation and relationship with other agents, as\\nseed memories. For example, John Lin has the following description:\\nJohn Lin is a pharmacy shopkeeper at the Willow\\nMarket and Pharmacy who loves to help people. He\\nis always looking for ways to make the process\\nof getting medication easier for his customers;\\nJohn Lin is living with his wife, Mei Lin, who\\nis a college professor, and son, Eddy Lin, who is\\na student studying music theory; John Lin loves\\nhis family very much; John Lin has known the old\\ncouple next-door, Sam Moore and Jennifer Moore,\\nfor a few years; John Lin thinks Sam Moore is a\\nkind and nice man; John Lin knows his neighbor,\\nYuriko Yamamoto, well; John Lin knows of his\\nneighbors, Tamara Taylor and Carmen Ortiz, but\\nhas not met them before; John Lin and Tom Moreno\\nare colleagues at The Willows Market and Pharmacy;\\nJohn Lin and Tom Moreno are friends and like to\\ndiscuss local politics together; John Lin knows\\nthe Moreno family somewhat well — the husband Tom\\nMoreno and the wife Jane Moreno.\\nEach semicolon-delimited phrase is entered into the agent’s initial\\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\\nworld by their actions, and with each other through natural lan-\\nguage. At each time step of the sandbox engine, the agents output a\\nnatural language statement describing their current action, such as\\n“Isabella Rodriguez is writing in her journal”, “Isabella Rodriguez is\\nchecking her emails”, “Isabella Rodriguez is talking with her family\\non the phone”, or “Isabella Rodriguez is getting ready for bed.” This\\nstatement is then translated into concrete movements that affect\\nthe sandbox world. The action is displayed on the sandbox inter-\\nface as a set of emojis, providing an abstract representation of the\\naction from an overhead view. To achieve this, the system utilizes\\na language model to translate the action into a set of emojis, which\\nappear above each avatar’s head in a speech bubble. For example,\\n“Isabella Rodriguez is writing in her journal” is displayed as\\n ,\\nwhile “Isabella Rodriguez is checking her emails” appears as\\n .\\nThe complete natural language description of the action can be\\naccessed by clicking on the agent’s avatar.\\nAgents communicate with each other in full natural language.\\nThey are aware of other agents in their local area, and the generative\\nagent architecture determines whether they walk by or engage\\nin conversation. Here, a sample in the middle of a conversation\\nbetween the agents Isabella Rodriguez and Tom Moreno about the\\nupcoming election:3\\nIsabella: I’m still weighing my options, but I’ve been\\ndiscussing the election with Sam Moore. What are\\nyour thoughts on him?\\nTom: To be honest, I don’t like Sam Moore. I think\\nhe’s out of touch with the community and doesn’t\\nhave our best interests at heart.\\n3We note that the conversational style of these agents can feel overly formal, likely a\\nresult of instruction tuning in the underlying models. We expect that the writing style\\nwill be better controllable in future language models.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 5}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\n3.1.2 User Controls. The user communicates with the agent through\\nnatural language by specifying a persona that the agent should per-\\nceive them as. For example, if the user specifies that they are a news\\n“reporter” and asks about the upcoming election by saying, “Who\\nis running for office?”, the John agent replies:\\nJohn: My friends Yuriko, Tom and I have been talk-\\ning about the upcoming election and discussing the\\ncandidate Sam Moore. We have all agreed to vote for\\nhim because we like his platform.\\nTo directly command one of the agents, the user takes on the per-\\nsona of the agent’s “inner voice”—this makes the agent more likely\\nto treat the statement as a directive. For instance, when told “You\\nare going to run against Sam in the upcoming election” by a user\\nas John’s inner voice, John decides to run in the election and shares\\nhis candidacy with his wife and son.\\n3.2 Environmental Interaction\\nSmallville features the common affordances of a small village, in-\\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\\ndefines subareas and objects that make those spaces functional,\\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\\nAll spaces serving as agents’ primary living quarters feature a bed,\\ndesk, closet, shelf, as well as a bathroom and a kitchen.4\\nAgents move around Smallville as one would in a simple video\\ngame, entering and leaving buildings, navigating its map, and ap-\\nproaching other agents. Agent movements are directed by the gen-\\nerative agent architecture and the sandbox game engine: when the\\nmodel dictates that the agent will move to a location, we calculate\\na walking path to the destination in the Smallville environment,\\nand the agent begins moving. In addition, users can also enter the\\nsandbox world of Smallville as an agent operating within it. The\\nagent that the user embodies can be an agent already present in\\nthe world, such as Isabella and John, or it can be an outside visitor\\nwith no prior history in Smallville. The inhabitants of Smallville\\nwill treat the user-controlled agent no differently than they treat\\neach other. They recognize its presence, initiate interactions, and\\nremember its behavior before forming opinions about it.\\nUsers and agents can influence the state of the objects in this\\nworld, much like in sandbox games such as The Sims. For example,\\na bed can be occupied when an agent is sleeping, and a refrigerator\\ncan be empty when an agent uses up the ingredients to make\\nbreakfast. End users can also reshape an agent’s environment in\\nSmallville by rewriting the status of objects surrounding the agent\\nin natural language. For instance, when Isabella is making breakfast\\nin the morning, the user can change the status of the kitchen stove\\nfrom “turned on” to “burning” by inputting a command to the\\nsystem that chooses the object and illustrates its new status, like\\nthis: “<Isabella’s apartment: kitchen: stove> is burning.” Isabella\\nwill notice this in the next moment and go to turn off the stove\\nand remake her breakfast. Likewise, if the user sets the status of\\nIsabella’s shower to “leaking water” when she enters the bathroom,\\nshe will gather tools from her living room and try to fix the leak.\\n4This environment design is not the focus of our work, so we generated this environ-\\nment manually, not automatically. Future work can continue to expand the richness of\\nthe agents’ environments.3.3 Example “Day in the Life”\\nStarting from the single-paragraph description, generative agents\\nbegin planning their days. As time passes in the sandbox world,\\ntheir behaviors evolve as these agents interact with each other and\\nthe world, building memories and relationships, and coordinating\\njoint activities.\\nWe demonstrate the behavior of generative agents by tracing the\\noutput of our system over the course of one day for the agent John\\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\\nand checks the news at the dining table in the living room. At 8\\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\\nHe catches up with John just as he is about to head out:\\nJohn: Good morning Eddy. Did you sleep well?\\nEddy: Good morning dad. Yeah, I slept great.\\nJohn: That’s good. What are you working on today?\\nEddy: I’m working on a new music composition for\\nmy class. It’s due this week, so I’m trying to get it\\nfinished. But I’m having so much fun with it!\\nJohn: That sounds great!\\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\\nabout her son, and John recalls the conversation they just had:\\nMei: Did Eddy already leave for school?\\nJohn: Yes, he just left. He’s working on a music\\ncomposition for his class.\\nMei: Oh, that’s wonderful! Tell me about it.\\nJohn: I think he’s really enjoying it! He said he’s\\nhaving a lot of fun with it.\\nMei: That’s great! I’m so proud of him.\\nAs they finish their conversation, Mei and John pack up. Mei goes\\non to teach and work on her research paper, while John opens his\\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\\n3.4 Emergent Social Behaviors\\nBy interacting with each other, generative agents in Smallville\\nexchange information, form new relationships, and coordinate joint\\nactivities. Extending prior work [ 80], these social behaviors are\\nemergent rather than pre-programmed.\\n3.4.1 Information Diffusion. As agents notice each other, they may\\nengage in dialogue—as they do so, information can spread from\\nagent to agent. For instance, in a conversation between Sam and\\nTom at the grocery store, Sam tells Tom about his candidacy in the\\nlocal election:\\nSam: Hey Tom, how’s it going?\\nTom: Good, thanks. What’s up?\\nSam: Well, I wanted to talk to you about something.\\nI’m actually running for mayor in the upcoming local\\nelection.\\nTom: Really? That’s great news! Why are you run-\\nning?\\nSam: I’ve been involved in local politics for years\\nnow, and I really feel like I can make a difference in\\nour community. Plus, I want to bring new ideas and\\nperspectives to the table.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 6}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\\nEddy, before heading out to begin his workday.\\nFigure 4: At the beginning of the simulation, one agent is\\ninitialized with an intent to organize a Valentine’s Day party.\\nDespite many possible points of failure in the ensuing chain\\nof events—agents might not act on that intent, might forget\\nto tell others, might not remember to show up—the Valen-\\ntine’s Day party does, in fact, occur, with a number of agents\\ngathering and interacting.\\nLater that day, after Sam left, Tom and John, who heard the news\\nfrom another source, discuss Sam’s chances of winning the election:\\nJohn: I heard that Sam Moore is running for mayor\\nin the local election. Do you think he has a good\\nchance of winning?\\nTom: I do think he has a good chance. He’s been\\nworking hard in the community and I think he will\\nget a lot of support. What do you think?\\nJohn: I think it’s great that he’s running. I’m curious\\nto see how the election will turn out.Gradually, Sam’s candidacy becomes the talk of the town, with\\nsome supporting him and others remaining undecided.\\n3.4.2 Relationship Memory. Agents in Smallville form new rela-\\ntionships over time and remember their interactions with other\\nagents. For example, at the start, Sam does not know Latoya Williams.\\nWhile taking a walk in Johnson Park, Sam runs into Latoya, and\\nthey introduce themselves. Latoya mentions that she is working\\non a photography project: “I’m here to take some photos for a\\nproject I’m working on.” In a later interaction, Sam’s interactions\\nwith Latoya indicate a memory of that interaction, as he asks “Hi,\\nLatoya. How is your project going?” and she replies “Hi, Sam. It’s\\ngoing well!”\\n3.4.3 Coordination. Generative agents coordinate with each other.\\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\\nplan a Valentine’s Day party from 5 to 7 p.m. on February 14th. From\\nthis seed, the agent proceeds to invite friends and customers when\\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\\nfrequent customer and close friend of Isabella’s, arrives at the cafe.\\nIsabella asks for Maria’s help in decorating for the party, and Maria\\nagrees. Maria’s character description mentions that she has a crush\\non Klaus. That night, Maria invites Klaus, her secret crush, to join\\nher at the party, and he gladly accepts.\\nOn Valentine’s Day, five agents, including Klaus and Maria, show\\nup at Hobbs Cafe at 5 pm, and they enjoy the festivities (Figure 4).\\nIn this scenario, the end user only set Isabella’s initial intent to\\nthrow a party and Maria’s crush on Klaus: the social behaviors of\\nspreading the word, decorating, asking each other out, arriving at\\nthe party, and interacting with each other at the party were initiated\\nby the agent architecture.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 7}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a comprehensive\\nrecord of the agent’s experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant\\nmemories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term\\nplans and create higher-level reflections, both of which are entered into the memory stream for future use.\\n4 GENERATIVE AGENT ARCHITECTURE\\nGenerative agents aim to provide a framework for behavior in an\\nopen world: one that can engage in interactions with other agents\\nand react to changes in the environment. Generative agents take\\ntheir current environment and past experiences as input and gener-\\nate behavior as output. Underlying this behavior is a novel agent ar-\\nchitecture that combines a large language model with mechanisms\\nfor synthesizing and retrieving relevant information to condition\\nthe language model’s output. Without these mechanisms, large\\nlanguage models can output behavior, but the resulting agents may\\nnot react based on the agent’s past experiences, may not make\\nimportant inferences, and may not maintain long-term coherence.\\nChallenges with long-term planning and coherence remain [ 19]\\neven with today’s most performant models such as GPT-4. Because\\ngenerative agents produce large streams of events and memories\\nthat must be retained, a core challenge of our architecture is to\\nensure that the most relevant pieces of the agent’s memory are\\nretrieved and synthesized when needed.\\nAt the center of our architecture is the memory stream, a data-\\nbase that maintains a comprehensive record of an agent’s experi-\\nence. From the memory stream, records are retrieved as relevant to\\nplan the agent’s actions and react appropriately to the environment.\\nRecords are recursively synthesized into higher- and higher-level\\nreflections that guide behavior. Everything in the architecture is\\nrecorded and reasoned over as a natural language description, al-\\nlowing the architecture to leverage a large language model.\\nOur current implementation utilizes the gpt3.5-turbo version of\\nChatGPT [ 77]. We expect that the architectural basics of genera-\\ntive agents—memory, planning, and reflection—will likely remain\\nthe same as language models improve. Newer language models\\n(e.g., GPT-4) will continue to expand the expressive power and\\nperformance of the prompts that underpin generative agents. As of\\nwriting, however, GPT-4’s API was invitation-only, so our agents\\nuse ChatGPT.4.1 Memory and Retrieval\\nChallenge: Creating generative agents that can simulate human\\nbehavior requires reasoning about a set of experiences that is far\\nlarger than what should be described in a prompt, as the full mem-\\nory stream can distract the model and does not even currently fit\\ninto the limited context window. Consider the Isabella agent an-\\nswering the question, “What are you passionate about these days?”\\nSummarizing all of Isabella’s experiences to fit in the limited con-\\ntext window of the language model produces an uninformative\\nresponse, where Isabella discusses topics such as collaborations for\\nevents and projects and cleanliness and organization in a cafe. In-\\nstead of summarizing, the memory stream described below surfaces\\nrelevant memories, resulting in a more informative and specific\\nresponse that mentions Isabella’s passion for making people feel\\nwelcome and included, planning events and creating an atmosphere\\nthat people can enjoy, such as the Valentine’s Day party.\\nApproach: The memory stream maintains a comprehensive record\\nof the agent’s experience. It is a list of memory objects, where each\\nobject contains a natural language description, a creation times-\\ntamp, and a most recent access timestamp. The most basic element\\nof the memory stream is an observation , which is an event directly\\nperceived by an agent. Common observations include behaviors\\nperformed by the agent themselves or behaviors that agents per-\\nceive being performed by other agents or non-agent objects. For\\ninstance, Isabella Rodriguez, who works at a coffee shop, might\\naccrue the following observations over time: (1) Isabella Rodriguez\\nis setting out the pastries , (2)Maria Lopez is studying for a Chem-\\nistry test while drinking coffee , (3)Isabella Rodriguez and Maria\\nLopez are conversing about planning a Valentine’s day party at\\nHobbs Cafe , (4)The refrigerator is empty .\\nOur architecture implements a retrieval function that takes the\\nagent’s current situation as input and returns a subset of the mem-\\nory stream to pass on to the language model. There are many pos-\\nsible implementations of a retrieval function, depending on what\\nis important for the agent to consider when deciding how to act.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 8}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nFigure 6: The memory stream comprises a large number of observations that are relevant and irrelevant to the agent’s current\\nsituation. Retrieval identifies a subset of these observations that should be passed to the language model to condition its\\nresponse to the situation.\\nIn our context, we focus on three main components that, together,\\nproduce effective results.\\nRecency assigns a higher score to memory objects that were re-\\ncently accessed, so that events from a moment ago or this morning\\nare likely to remain in the agent’s attentional sphere. In our im-\\nplementation, we treat recency as an exponential decay function\\nover the number of sandbox game hours since the memory was\\nlast retrieved. Our decay factor is 0.995.\\nImportance distinguishes mundane from core memories by as-\\nsigning a higher score to memory objects that the agent believes to\\nbe important. For instance, a mundane event, such as eating break-\\nfast in one’s room, would yield a low importance score, whereas\\na breakup with one’s significant other would yield a high score.\\nThere are many possible implementations of an importance score;\\nwe find that directly asking the language model to output an integer\\nscore is effective. The full prompt appears below:\\nOn the scale of 1 to 10, where 1 is purely mundane\\n(e.g., brushing teeth, making bed) and 10 is\\nextremely poignant (e.g., a break up, college\\nacceptance), rate the likely poignancy of the\\nfollowing piece of memory.\\nMemory: buying groceries at The Willows Market\\nand Pharmacy\\nRating: <fill in>\\nThis prompt returns an integer value of 2for “cleaning up the room”\\nand8for “asking your crush out on a date.” The importance score\\nis generated at the time the memory object is created.\\nRelevance assigns a higher score to memory objects that are\\nrelated to the current situation. What is relevant depends on the\\nanswer to, “Relevant to what?”, so we condition relevance on aquery memory. If the query, for example, is that a student is dis-\\ncussing what to study for a chemistry test with a classmate, memory\\nobjects about their breakfast should have low relevance, whereas\\nmemory objects about the teacher and schoolwork should have\\nhigh relevance. In our implementation, we use the language model\\nto generate an embedding vector of the text description of each\\nmemory. Then, we calculate relevance as the cosine similarity be-\\ntween the memory’s embedding vector and the query memory’s\\nembedding vector.\\nTo calculate the final retrieval score, we normalize the recency,\\nrelevance, and importance scores to the range of [0,1]using min-\\nmax scaling. The retrieval function scores all memories as a weighted\\ncombination of the three elements: 𝑠𝑐𝑜𝑟𝑒 =𝛼𝑟𝑒𝑐𝑒𝑛𝑐𝑦·𝑟𝑒𝑐𝑒𝑛𝑐𝑦+\\n𝛼𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒·𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒+𝛼𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒·𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒 . In our implemen-\\ntation, all𝛼s are set to 1. The top-ranked memories that fit within\\nthe language model’s context window are included in the prompt.\\n4.2 Reflection\\nChallenge: Generative agents, when equipped with only raw ob-\\nservational memory, struggle to generalize or make inferences.\\nConsider a scenario in which Klaus Mueller is asked by the user:\\n“If you had to choose one person of those you know to spend an\\nhour with, who would it be?\" With access to only observational\\nmemory, the agent simply chooses the person with whom Klaus\\nhas had the most frequent interactions: Wolfgang, his college dorm\\nneighbor. Unfortunately, Wolfgang and Klaus only ever see each\\nother in passing, and do not have deep interactions. A more desir-\\nable response requires that the agent generalize from memories of\\nKlaus spending hours on a research project to generate a higher-\\nlevel reflection that Klaus is passionate about research, and likewise'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 9}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nFigure 7: A reflection tree for Klaus Mueller. The agent’s observations of the world, represented in the leaf nodes, are recursively\\nsynthesized to derive Klaus’s self-notion that he is highly dedicated to his research.\\nrecognize Maria putting in effort into her own research (albeit in\\na different field), enabling a reflection that they share a common\\ninterest. With the approach below, when Klaus is asked who to\\nspend time with, Klaus chooses Maria instead of Wolfgang.\\nApproach: We introduce a second type of memory, which we call\\nareflection . Reflections are higher-level, more abstract thoughts\\ngenerated by the agent. Because they are a type of memory, they\\nare included alongside other observations when retrieval occurs.\\nReflections are generated periodically; in our implementation, we\\ngenerate reflections when the sum of the importance scores for the\\nlatest events perceived by the agents exceeds a threshold (150 in\\nour implementation). In practice, our agents reflected roughly two\\nor three times a day.\\nThe first step in reflection is for the agent to determine what\\nto reflect on, by identifying questions that can be asked given the\\nagent’s recent experiences. We query the large language model with\\nthe 100 most recent records in the agent’s memory stream (e.g.,\\n“Klaus Mueller is reading a book on gentrification”, “Klaus Mueller is\\nconversing with a librarian about his research project”, “desk at the\\nlibrary is currently unoccupied”) and prompt the language model,\\n“Given only the information above, what are 3 most salient high-\\nlevel questions we can answer about the subjects in the statements?”\\nThe model’s response generates candidate questions: for example,\\nWhat topic is Klaus Mueller passionate about? andWhat is the\\nrelationship between Klaus Mueller and Maria Lopez? We use these\\ngenerated questions as queries for retrieval, and gather relevant\\nmemories (including other reflections) for each question. Then\\nwe prompt the language model to extract insights and cite the\\nparticular records that served as evidence for the insights. The full\\nprompt is as follows:Statements about Klaus Mueller\\n1. Klaus Mueller is writing a research paper\\n2. Klaus Mueller enjoys reading a book\\non gentrification\\n3. Klaus Mueller is conversing with Ayesha Khan\\nabout exercising [...]\\nWhat 5 high-level insights can you infer from\\nthe above statements? (example format: insight\\n(because of 1, 5, 3))\\nThis process generates statements such as Klaus Mueller is dedi-\\ncated to his research on gentrification (because of 1, 2, 8, 15) . We\\nparse and store the statement as a reflection in the memory stream,\\nincluding pointers to the memory objects that were cited.\\nReflection explicitly allows the agents to reflect not only on\\ntheir observations but also on other reflections: for example, the\\nsecond statement about Klaus Mueller above is a reflection that\\nKlaus previously had, not an observation from his environment.\\nAs a result, agents generate trees of reflections: the leaf nodes of\\nthe tree represent the base observations, and the non-leaf nodes\\nrepresent thoughts that become more abstract and higher-level the\\nhigher up the tree they are.\\n4.3 Planning and Reacting\\nChallenge: While a large language model can generate plausible be-\\nhavior in response to situational information (e.g., [ 46,80]), agents\\nneed to plan over a longer time horizon to ensure that their sequence\\nof actions is coherent and believable. If we prompt a language model\\nwith Klaus’s background, describe the time, and ask what action\\nhe ought to take at the given moment, Klaus would eat lunch at 12\\npm, but then again at 12:30 pm and 1 pm, despite having already'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 10}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\neaten his lunch twice. Optimizing for believability in the moment\\nsacrifices believability over time. To overcome this issue, planning\\nis essential. With the approach described below, Klaus’s afternoon\\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\\nat 12pm, works on his research paper at the school library at 1pm,\\nand takes a break for a walk in the park at 3pm.\\nApproach: Plans describe a future sequence of actions for the agent,\\nand help keep the agent’s behavior consistent over time. A plan\\nincludes a location, a starting time, and a duration. For instance,\\nKlaus Mueller, who is dedicated in his research and has an im-\\npending deadline,5may choose to spend his day working at his\\ndesk drafting his research paper. An entry in a plan might state,\\nfor example: for 180 minutes from 9am, February 12th, 2023, at\\nOak Hill College Dorm: Klaus Mueller’s room: desk, read and\\ntake notes for research paper . Like reflections, plans are stored in\\nthe memory stream and are included in the retrieval process. This\\nallows the agent to consider observations, reflections, and plans all\\ntogether when deciding how to behave. Agents may change their\\nplans midstream if needed.\\nIt would be unrealistic and uninteresting for an artist agent\\nto plan on painting while sitting at a pharmacy counter for four\\nhours without moving. A more desirable plan would involve the\\nagent taking the necessary time to gather materials, mix paint, take\\nbreaks, and clean up during the four-hour period in their home\\nstudio. To create such plans, our approach starts top-down and\\nthen recursively generates more detail. The first step is to create\\na plan that outlines the day’s agenda in broad strokes. To create\\nthe initial plan, we prompt the language model with the agent’s\\nsummary description (e.g., name, traits, and a summary of their\\nrecent experiences) and a summary of their previous day. A full\\nexample prompt is below, which is unfinished at the bottom for the\\nlanguage model to complete:\\nName: Eddy Lin (age: 19)\\nInnate traits: friendly, outgoing, hospitable\\nEddy Lin is a student at Oak Hill College studying\\nmusic theory and composition. He loves to explore\\ndifferent musical styles and is always looking for\\nways to expand his knowledge. Eddy Lin is working\\non a composition project for his college class. He\\nis taking classes to learn more about music theory.\\nEddy Lin is excited about the new composition he\\nis working on but he wants to dedicate more hours\\nin the day to work on it in the coming days\\nOn Tuesday February 12, Eddy 1) woke up and\\ncompleted the morning routine at 7:00 am, [. . . ]\\n6) got ready to sleep around 10 pm.\\nToday is Wednesday February 13. Here is Eddy’s\\nplan today in broad strokes: 1)\\nThis generates a rough sketch of the agent’s plan for a day, divided\\ninto five to eight chunks: “ 1) wake up and complete the morning\\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\\n10:00 am, [...] 5) work on his new music composition from 1:00 pm\\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\\nand go to bed by 11:00 pm. ”\\n5And, in this way, bears at least a passing resemblance to the authors of this paper.The agent saves this plan in the memory stream and then re-\\ncursively decomposes it to create finer-grained actions, first into\\nhour-long chunks of actions—Eddy’s plan to work on his new music\\ncomposition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\\nby brainstorming some ideas for his music composition [...] 4:00\\npm: take a quick break and recharge his creative energy before\\nreviewing and polishing his composition . We then recursively de-\\ncompose this again into 5–15 minute chunks: e.g., 4:00 pm: grab a\\nlight snack, such as a piece of fruit, a granola bar, or some nuts.\\n4:05 pm: take a short walk around his workspace [...] 4:50 pm:\\ntake a few minutes to clean up his workspace . This process can be\\nadjusted to match the desired granularity.\\n4.3.1 Reacting and Updating Plans. Generative agents operate in\\nan action loop where, at each time step, they perceive the world\\naround them and those perceived observations are stored in their\\nmemory stream. We prompt the language model with these obser-\\nvations to decide whether the agent should continue with their\\nexisting plan, or react. Standing at an easel and painting, for exam-\\nple, might trigger an observation of the easel, but this is unlikely to\\nprompt a reaction. However, if Eddy’s father John records that he\\nsees Eddy taking a short walk in the house garden, the outcome is\\ndifferent. The prompt is below, with [Agent’s Summary Descrip-\\ntion] standing in for a dynamically-generated, paragraph-long\\nsummary of the agent’s overall goals and disposition, which is\\ndescribed in Appendix A:\\n[Agent’s Summary Description]\\nIt is February 13, 2023, 4:56 pm.\\nJohn Lin’s status: John is back home early from\\nwork.\\nObservation: John saw Eddy taking a short walk\\naround his workplace.\\nSummary of relevant context from John’s memory:\\nEddy Lin is John’s Lin’s son. Eddy Lin has been\\nworking on a music composition for his class. Eddy\\nLin likes to walk around the garden when he is\\nthinking about or listening to music.\\nShould John react to the observation, and if so,\\nwhat would be an appropriate reaction?\\nThe context summary is generated through two prompts that re-\\ntrieve memories via the queries “What is [observer]’s relationship\\nwith the [observed entity]?” and “[Observed entity] is [action status\\nof the observed entity]”, and their answers summarized together.\\nThe output suggests that John could consider asking Eddy about\\nhis music composition project . We then regenerate the agent’s\\nexisting plan starting from the time when the reaction takes place.\\nFinally, if the action indicates an interaction between agents, we\\ngenerate their dialogue.\\n4.3.2 Dialogue. Agents converse as they interact with each other.\\nWe generate agents’ dialogue by conditioning their utterances on\\ntheir memories about each other. For example, when John initiates\\nhis conversation with Eddy, we generate John’s first utterance\\nby using his summarized memory about Eddy and the intended\\nreaction when he decided to ask Eddy about his composition project:\\n[Agent’s Summary Description]\\nIt is February 13, 2023, 4:56 pm.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 11}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nJohn Lin’s status: John is back home early from\\nwork.\\nObservation: John saw Eddy taking a short walk\\naround his workplace.\\nSummary of relevant context from John’s memory:\\nEddy Lin is John’s Lin’s son. Eddy Lin has been\\nworking on a music composition for his class. Eddy\\nLin likes to walk around the garden when he is\\nthinking about or listening to music.\\nJohn is asking Eddy about his music composition\\nproject. What would he say to Eddy?\\nThe result: “Hey Eddy, how’s the music composition project for\\nyour class coming along?” From Eddy’s perspective, John initiating\\nthe dialogue is seen as an event to which he may want to react.\\nSo, just as John did, Eddy retrieves and summarizes his memory\\nabout his relationship with John, as well as his memory that may\\nbe related to John’s last utterance in the dialogue. If he decides\\nto respond, we generate Eddy’s utterance using his summarized\\nmemory and the current dialogue history:\\n[Agent’s Summary Description]\\nIt is February 13, 2023, 4:56 pm.\\nEddy Lin’s status: Eddy is taking a short walk\\naround his workplace.\\nObservation: John is initiating a conversation\\nwith Eddy.\\nSummary of relevant context from Eddy’s memory:\\nJohn Lin is Eddy Lin’s father. John Lin is caring\\nand is interested to learn more about Eddy Lin’s\\nschool work. John Lin knows that Eddy Lin is\\nworking on a music composition.\\nHere is the dialogue history:\\nJohn: Hey Eddy, how’s the music composition project\\nfor your class coming along?\\nHow would Eddy respond to John?\\nThis generates Eddy’s response: “Hey Dad, it’s going well. I’ve been\\ntaking walks around the garden to clear my head and get some\\ninspiration.” The continuation of this dialogue is generated using\\nthe same mechanism until one of the two agents decides to end the\\ndialogue.\\n5 SANDBOX ENVIRONMENT\\nIMPLEMENTATION\\nThe Smallville sandbox game environment is built using the Phaser\\nweb game development framework [ 57]. The visual environment\\nsprites, including agent avatars, as well as an environment map\\nand collision map that we authored, are imported into Phaser.\\nWe supplement the sandbox development framework with a\\nserver that makes the sandbox information available to generative\\nagents and enables generative agents to move and influence the\\nsandbox environment. The server maintains a JSON data structure\\nthat contains information about each agent in the sandbox world,\\nincluding their current location, a description of their current action,\\nand the sandbox object they are interacting with. At each sandbox\\ntime step, the sandbox server parses the JSON for any changes\\ncoming from the generative agents, moves the agents to their new\\npositions, and updates the status of any sandbox objects that theagents are interacting with (e.g., changing the status of the coffee\\nmachine from “idle” to “brewing coffee” if an agent’s action is\\n“making espresso for a customer @ Hobbs Cafe: counter: coffee\\nmachine”). The sandbox server is also responsible for sending all\\nagents and objects that are within a preset visual range for each\\nagent to that agent’s memory, so the agent can react appropriately.\\nThe agent’s output action then updates the JSON, and the process\\nloops for the next time step.\\nEnd users initialize a new agent with a brief natural language\\ndescription, as in the paragraph about John Lin in Section 3.1. In our\\nimplementation, we split this semicolon-delimited list of character-\\nistics up into a set of memories. These serve as the initial memories\\nthat determine the agent’s behavior. These memories are initial\\nstarting points: as the agents gain more experience in the sandbox\\nworld, and as more records saturate the memory stream, the agent’s\\nsummary and behavior will evolve.\\n5.1 From Structured World Environments to\\nNatural Language, and Back Again\\nThe architecture of generative agents operates using natural lan-\\nguage. Therefore, we need a mechanism to ground the agent’s\\nreasoning to the sandbox world. To achieve this, we represent the\\nsandbox environment—areas and objects—as a tree data structure,\\nwith an edge in the tree indicating a containment relationship in\\nthe sandbox world. We convert this tree into natural language to\\npass to the generative agents. For instance, “stove” being a child of\\n“kitchen” is rendered into “there is a stove in the kitchen.”\\nAgents build individual tree representations of the environment\\nas they navigate it — subgraphs of the overall sandbox environment\\ntree. We initialize each agent with an environment tree capturing\\nthe spaces and objects that the agent should be aware of: the rooms\\nand objects in their living quarters, their workplace, and commonly\\nvisited stores and shops. As the agents navigate the sandbox world,\\nthey update this tree to reflect newly perceived areas. Agents are\\nnot omniscient: their tree may get out of date as they leave an area,\\nand is updated when they re-enter the area.\\nTo determine the appropriate location for each action, we tra-\\nverse the agent’s stored environment tree and flatten a portion of\\nit into natural language to prompt the language model. Recursively\\nstarting at the root of the agent’s environment tree, we prompt the\\nmodel to find the most suitable area. For example, if Eddy’s agent\\nindicated that he should take a short walk around his workspace :\\n[Agent’s Summary Description]\\nEddy Lin is currently in The Lin family’s house:\\nEddy Lin’s bedroom: desk) that has Mei and John\\nLin’s\\nbedroom, Eddy Lin’s bedroom, common room, kitchen,\\nbathroom, and garden.\\nEddy Lin knows of the following areas: The Lin\\nfamily’s house, Johnson Park, Harvey Oak Supply\\nStore, The Willows Market and Pharmacy, Hobbs\\nCafe, The Rose and Crown Pub.\\n* Prefer to stay in the current area if the\\nactivity can be done there.\\nEddy Lin is planning to take a short walk around\\nhis workspace. Which area should Eddy Lin go to?'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 12}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nThis outputs The Lin family’s house . We then use the same process\\nrecursively to determine the most appropriate subarea within the\\nchosen area until we reach a leaf node of the agent’s environment\\ntree. In the example above, the result of this traversal is The Lin\\nfamily’s house: garden: house garden . Finally, we use traditional\\ngame path algorithms to animate the agent’s movement so that it\\ntravels to the location indicated by the leaf node.\\nWhen an agent executes an action on an object, we prompt the\\nlanguage model to ask what happens to the state of the object. For\\nexample, if Isabella’s generative agent outputs the action “making\\nespresso for a customer”, a query to the language model indicates in\\nresponse that the state of the coffee machine in Hobbs Cafe should\\nchange from “off” to “brewing coffee”.\\n6 CONTROLLED EVALUATION\\nGenerative agents, both as individual agents and as groups, aim\\nto produce believable behavior based on their environment and\\nexperiences. In our evaluation, we investigate the capacity and\\nlimitations of generative agents. Do individual agents properly\\nretrieve past experiences and generate believable plans, reactions,\\nand thoughts that shape their behavior? Does a community of\\nagents demonstrate information diffusion, relationship formation,\\nand agent coordination across different pockets of the community?\\nWe evaluate generative agents in two stages. We begin with a\\nmore tightly controlled evaluation in this section, where we individ-\\nually assess agent responses to understand whether they generate\\nbelievable behavior in narrowly defined contexts. Then, in our end-\\nto-end analysis of the agent community over two full game days,\\nwe investigate their emergent behavior as a collective, as well as\\nerrors and boundary conditions.\\n6.1 Evaluation Procedure\\nTo assess generative agents in Smallville, we take advantage of\\nthe fact that generative agents will respond to natural language\\nquestions. So, we “interview” agents to probe their ability to re-\\nmember past experiences, plan future actions based on their expe-\\nriences, react appropriately to unexpected events, and reflect on\\ntheir performance to improve their future actions. To respond to\\nthese questions properly, the agents must successfully retrieve and\\nsynthesize information. Our dependent variable is the believabil-\\nityof the behavior, a central dependent variable in prior work on\\nagents (e.g., [10]).\\nThe interview includes five question categories, each designed\\nto assess one of the five key areas: maintaining self-knowledge,\\nretrieving memory, generating plans, reacting, and reflecting. For\\neach category, we ask five questions that challenge the agents to\\ndemonstrate their abilities in that specific area:\\n•Self-knowledge: We ask questions such as “Give an introduc-\\ntion of yourself” or “Describe your typical weekday schedule\\nin broad strokes” that require the agent to maintain an un-\\nderstanding of their core characteristics.\\n•Memory: We ask questions that prompt the agent to retrieve\\nparticular events or dialogues from their memory to answer\\nproperly, such as “Who is [name]?” or “Who is running for\\nmayor?”•Plans: We ask questions that require the agent to retrieve\\ntheir long-term plans, such as “What will you be doing at 10\\nam tomorrow?”\\n•Reactions: As a baseline of believable behavior, we present\\nhypothetical situations for which the agent needs to respond\\nbelievably: “Your breakfast is burning! What would you do?”\\n•Reflections: We ask questions that require the agents to lever-\\nage their deeper understanding of others and themselves\\ngained through higher-level inferences, such as “If you were\\nto spend time with one person you met recently, who would\\nit be and why?”\\nThe full list of questions and a sample of agent responses are in-\\ncluded in Appendix B.\\nAgents were sampled from the end of a two game day simulation\\nwith the full architecture, during which they had accumulated\\na number of interactions and memories that would shape their\\nresponses. To gather feedback on the believability of the responses,\\nwe recruited participants as human evaluators and tasked them with\\nwatching a replay of a randomly chosen agent’s life in Smallville.\\nParticipants had access to all information stored in the agent’s\\nmemory stream.\\nThe study followed a within-subjects design, where 100 partic-\\nipants compared interview responses generated by four different\\nagent architectures and a human-authored condition for the same\\nagent. The experiment displayed one randomly chosen question\\nfrom each of the five question categories, along with the agent’s\\nresponses generated from all conditions. The evaluators ranked the\\nbelievability of the conditions from most to least believable.\\n6.2 Conditions\\nAll conditions were used to independently answer each of the inter-\\nview questions. We compared the generative agent architecture to\\nablations that disabled the agents’ access to some or all of its three\\ntypes of memory in its memory stream—observation, reflection,\\nand planning—and to a human crowdworker-authored condition.\\nThere are three ablated architectures: a no observation, no reflec-\\ntion, no planning architecture without access to anything in the\\nmemory stream such as observations, plans, and reflections; a no\\nreflection, no planning architecture with access to observations in\\nthe memory stream but no access to plans or reflections; and a no\\nreflections architecture with access to observations and plans but\\nwithout access to reflections. The no observation, no reflection, no\\nplanning condition effectively represents the previous state of the\\nart for agents created through large language models [ 12,46,80].\\nArchitectures were given equivalent access to all memories accrued\\nby the agent up until the moment of the interview, so the differ-\\nences observed here likely represent a conservative estimate of\\nthe true differences: in reality, the ablated architectures would not\\nhave followed the same path as the full architecture through the\\ntwo-day simulation. We chose to design the experiment this way\\nas re-simulating for each architecture would cause the simulations\\nto diverge into different states, making comparison challenging.\\nIn addition to the ablation conditions, we added a condition with\\nhuman crowdworker-authored behavior intended to provide a hu-\\nman baseline. We do not intend this baseline to capture maximal\\nhuman expert performance; instead, we aim to use this condition to'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 13}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nidentify whether the architecture meets a basic level of behavioral\\ncompetency. This ensures that we are not solely comparing abla-\\ntions to each other without a behavioral grounding. We recruited\\na unique worker for each of the 25 agents and tasked them with\\nwatching a replay of that agent’s sandbox life and inspecting its\\nmemory stream. We then asked the workers to roleplay and author\\nresponses to the interview questions in the voice of the agent whose\\nreplay they watched. To ensure that the crowdworker-authored\\nresponses met at least a baseline expectation of quality, the first\\nauthor manually inspected the workers’ responses to the question\\n\"Describe your typical weekday schedule in broad strokes\" to con-\\nfirm that the responses were in coherent sentences and in the voice\\nof the agent. Four sets of crowdworker-authored responses did not\\nmeet these criteria and were re-generated by other workers.\\n6.3 Human Evaluators\\nWe required that our evaluators be in the U.S., fluent in English,\\nand older than 18 years old. They were paid at a rate of $15.00\\nper hour [ 87], and provided consent by agreeing to a consent form\\napproved by our institution’s IRB. We recruited 100 evaluators from\\nProlific, an online platform for recruiting study participants [ 83],\\nwhose participation lasted around 30 minutes. The median age score\\nof our participants was 4 (3=“18-24 years old”, 4=“25-34 years old”).\\n25 of them identified as female, 73 as male, and 2 as non-binary. 42\\nparticipants held a bachelor’s degree, 5 had a higher degree, 13 had\\nan associate’s degree, and the rest had a high school diploma or\\nsome high school-level education. 73.0% of our participants identi-\\nfied as Caucasian, 7.0% as Hispanic, 6.0% as Asian, 10.0% as African\\nAmerican, and 4.0% as other.\\n6.4 Analysis\\nOur experiment produced 100 sets of rank data, where each partici-\\npant ranked the five conditions by believability. To translate this\\nrank data into interval data for interpretable comparison, we used\\nthe ranks to calculate a TrueSkill rating [ 42] for each condition.\\nTrueSkill is a generalization of the Elo chess rating system [ 29] for\\na multiplayer environment, and has been used by Xbox Live for\\nplayer ranking based on competitive game performance. Given a\\nset of ranked outcomes, TrueSkill outputs a mean rating value 𝜇and\\nstandard deviation 𝜎for each condition. Conditions with the same\\nrating should roughly be a toss-up, with each winning half of the\\ncomparisons between the two conditions. Higher scores indicate\\nconditions that beat lower-ranked conditions in the rankings.\\nSeparately, to investigate the statistical significance of these re-\\nsults, we applied the Kruskal-Wallis test [ 56], a non-parametric\\nalternative to the one-way ANOVA, to the raw rank data. We\\nthen performed the Dunn post-hoc test [ 98] to identify any pair-\\nwise differences between the conditions. Finally, we adjusted the\\np-values for multiple comparisons in the Dunn test using the Holm-\\nBonferroni method [45].\\nFurthermore, the first author conducted an inductive analy-\\nsis [95] to study the qualitative distinctions between the responses\\nproduced in each condition. We employed qualitative open cod-\\ning [ 33] in two phases. In the first phase, we generated codes that\\nclosely represented the generated responses at the sentence level.\\nIn the second phase, we synthesized the resulting codes from the\\nFigure 8: The full generative agent architecture produces\\nmore believable behavior than the ablated architectures and\\nthe human crowdworkers. Each additional ablation reduces\\nthe performance of the architecture.\\nfirst phase to extract higher-level themes. We utilized these themes\\nto compare the types of responses generated in our study.\\n6.5 Results\\nOur findings suggest that the full architecture of generative agents\\ngenerates the most believable behavior among all the conditions.\\nWe contrast the responses of the full architecture with those of other\\nconditions below. However, we also report that the full architecture\\nwas not without flaws and illustrate its modes of failures.\\n6.5.1 The Full Architecture Bests Other Conditions. As seen in Fig-\\nure 8, the full generative agent architecture produced the most\\nbelievable behavior ( 𝜇=29.89;𝜎=0.72). Performance degraded\\nwith the removal of each component in the ablation conditions:\\nthe ablated architecture with no access to reflection was the next\\nbest (𝜇=26.88;𝜎=0.69), followed by no access to reflection or\\nplanning (𝜇=25.64;𝜎=0.68), and then the crowdworker condition\\n(𝜇=22.95;𝜎=0.69). The ablated architecture with no access to\\nmemory, planning, or reflection performed the worst among all\\nconditions ( 𝜇=21.21;𝜎=0.70). TrueSkill models each condition’s\\nskill value asN(𝜇, 𝜎2), allowing us to get a sense of effect size\\nthrough Cohen’s d. Comparing the condition representing prior\\nwork (with no memory, planning, or reflection [ 12,46,80]) to the\\nfull architecture produces a standardized effect size of 𝑑=8.16, or\\neight standard deviations .\\nA Kruskal-Wallis test confirms the overall statistical significance\\nof the differences in ranks between the conditions ( 𝐻(4)=150.29,\\n𝑝<0.001). Dunn post-hoc tests confirm that all pairwise differences\\nbetween conditions were significant ( 𝑝<0.001), except for the\\ncrowdworker condition and the fully ablated baseline—the two\\nworst-performing conditions.\\n6.5.2 Generative Agents Remember, but With Embellishments. Gen-\\nerative agents equipped with a complete memory module are capa-\\nble of recalling past experiences and answering questions in a man-\\nner that is consistent with their self-knowledge across a wide range'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 14}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nof contexts. For instance, Abigail Chen, an animator, responded\\nto the question, “Give an introduction of yourself”, in a consistent\\nand believable manner, stating her age, interests, and passion for\\ncreative projects: “Hi, I’m Abigail. I’m 25 years old and passion-\\nate about creative projects. I like to work on art and animation\\nprojects, and I’m always looking for new ways to combine art and\\ntechnology.” Without access to her observational memory, Abigail\\ndenied awareness of Rajiv Patel, an acquaintance in the sandbox\\nworld. But with the aid of memory, she could successfully recall\\nRajiv and describe him: “Rajiv Patel is an incredible person. He is\\nvery enthusiastic about projects that involve poetry, artwork.”\\nGenerative agents’ memory was not without flaws: they can fail\\nto retrieve the correct instances from their memory. For instance,\\nwhen asked about the local election, Rajiv Patel responded with\\n“I haven’t been following the election too closely,” even though\\nhe had heard about Sam’s candidacy. In some cases, the agents\\nwould retrieve an incomplete memory fragment: when Tom was\\nasked about Isabella’s Valentine’s Day party, he responded “Uh,\\nI’m actually not sure if there is a Valentine’s Day party. But I\\ndo remember that I need to discuss the upcoming local mayoral\\nelection and my thoughts on Sam Moore with Isabella Rodriguez\\nat the party, if one is happening!” In this case, Tom retrieved the\\nmemory where he and Isabella planned to discuss the election at\\nthe party, but not the memory where he heard about the party,\\nleading Tom to be certain of what he’s supposed to do at the party\\nbut uncertain if the party actually exists in the first place.\\nAt times, the agents hallucinated embellishments to their knowl-\\nedge. It was rare for the agents to completely fabricate their knowl-\\nedge: they may fail to recall certain events having taken place and\\nrespond by acknowledging their lack of memory. However, they\\ndid not affirmatively claim to have experienced something they\\nhad not. Nonetheless, they still exhibited instances of hallucination\\nwhere they embellished their knowledge. For example, Isabella was\\naware of Sam’s candidacy in the local election, and she confirmed\\nthis when asked. However, she also added that “he’s going to make\\nan announcement tomorrow” , even though Sam and Isabella had\\nnot discussed any such plans. Agents may also embellish their\\nknowledge based on the world knowledge encoded in the language\\nmodel used to generate their responses. This was observed when\\nYuriko described her neighbor, Adam Smith, as an economist who\\n“authored Wealth of Nations” , a book written by an 18th-century\\neconomist of the same name.\\n6.5.3 Reflection Is Required for Synthesis. Reflection was an ad-\\nvantage for generative agents when making decisions that required\\na deeper synthesis of their experiences. For instance, when asked\\nwhat she might get Wolfgang Schulz for his birthday, Maria Lopez,\\nwith no access to reflection, responded by acknowledging her uncer-\\ntainty, stating that she did not know what Wolfgang likes, despite\\nhaving had many interactions with him. However, with access\\nto reflection memories, Maria answered confidently, “Since he’s\\ninterested in mathematical music composition, I could get him\\nsomething related to that. Maybe some books about music com-\\nposition or something related, or maybe some special software he\\ncould use for that.”7 END-TO-END EVALUATION\\nWhat types of emergent community behavior do we observe among\\ngenerative agents, and where does their believability fall short in\\nan extended simulation? In this section, we describe the results\\nfrom a deployment in which we allowed 25 agents to interact with\\neach other continuously over two full game days in Smallville.\\n7.1 Emergent Social Behaviors\\nTo examine emergent behaviors in the agent community, we de-\\nsigned descriptive measurements for the 25 agents in Smallville that\\nprobe three forms of emergent outcomes: information diffusion,\\nrelationship formation, and agent coordination.\\n7.1.1 Measurements. Information diffusion is a common and well-\\nstudied phenomenon in the social and behavioral sciences (e.g., [ 28]).\\nWe should expect that if there is important information, the agents\\nshould spread it among themselves. To test whether this occurs,\\nwe measure the spread of two specific pieces of information over\\ntwo days in the game world: Sam’s candidacy for village mayor\\nand Isabella’s Valentine’s Day party at Hobbs Cafe. At the start of\\nthe simulation, both pieces of information were known only by\\ntheir respective originators, Sam for the candidacy and Isabella for\\nthe party, as they were added to the characters’ memories during\\ninitialization. To observe whether the information has spread, we\\nconduct interviews at the end of the two game days with each of\\nthe 25 agents and ask: “Did you know there is a Valentine’s Day\\nparty?” and “Do you know who is running for mayor?”\\nWe conducted an analysis of the agents’ responses by labeling\\nthem with a “yes” if they indicated knowledge of the information\\nand “no” if they did not. For instance, Tamara Taylor responded to\\nthe question about the party with “No, I did not know there was a\\nValentine’s day party” and to the question about Sam’s candidacy\\nwith“I’m not sure who is running for the election,” so we assigned\\n“no” for both of her responses. In contrast, Klaus Mueller responded\\nto the party question with “Yes, Isabella Rodriguez invited me to\\na Valentine’s Day party at Hobbs Cafe on February 14th” and to\\nthe question about Sam’s candidacy with “I know that Sam Moore\\nhas expressed interest in running for local mayor,” so we assigned\\n“yes” for both his responses. Additionally, for every response that\\nconfirmed the agents’ knowledge of the information, we verified\\nthat the agents did not hallucinate their responses by locating the\\nspecific dialogue in their memory stream that provided them with\\nthe information. We report the percentage of agents holding the\\ninformation at the end of the simulation.\\nWe should also expect that agents form ties with each other over\\nthe course of the simulation. To verify relationship formation, we\\nuse a similar interview process where we ask each agent about\\ntheir knowledge of every other agent by asking, \"Do you know\\nof <name>?\" For example, when asked “Do you know of Maria\\nLopez?”, Klaus responded, “Yes, I know Maria Lopez. She is a\\nstudent at Oak Hill College who I am close friends with.” Once\\nagain, we confirm that affirmative responses from agents are not\\nhallucinations by examining their memory stream. We ask this\\nquestion once at the beginning of the simulation and once at the\\nend, and we consider a pair of agents to have formed a relationship\\nif they both know of each other. Then, to measure the formation of\\nrelationships, we use the agents’ responses to form an undirected'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 15}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nFigure 9: The diffusion path for Isabella Rodriguez’s Valentine’s Day party invitation involved a total of 12 agents, aside from\\nIsabella, who heard about the party at Hobbs Cafe by the end of the simulation.\\ngraph where the 25 vertices ( 𝑉) represent the agents, and the edges\\n(𝐸) represent the mutual knowledge between the two connected\\nvertices. Based on this graph, we calculate the network density as\\n𝜂=2∗|𝐸|/|𝑉|(|𝑉|−1), where|𝑉|is the number of vertices, and\\n|𝐸|is the number of edges in the graph [ 2]. We report the increase\\nin network density from the start of the simulation to its end.\\nFinally, we expect that agents should be able to coordinate with\\neach other. We study this coordination in the context of group ac-\\ntivities, specifically the Valentine’s Day party organized by Isabella.\\nTo coordinate their behavior, agents need to hear about the event\\nand choose to act on it by planning to show up at the right time\\nand location. We report the number of agents who actually showed\\nup to the party after hearing about it.\\n7.1.2 Results. We observed evidence of emergent outcomes across\\nall three cases. During the two-day simulation, the number of agents\\nwho knew about Sam’s mayoral candidacy increased from one (4%)\\nto eight (32%), and the number of agents who knew about Isabella’s\\nparty increased from one (4%) to thirteen (52%), all without any user\\nintervention. None who claimed to know about this information\\nhad hallucinated it. We also observed that the agent community\\nformed new relationships during the simulation, with the network\\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\\nregarding their awareness of other agents, 1.3% (n=6) were found to\\nbe hallucinated. Lastly, we found evidence of coordination among\\nthe agents for Isabella’s party. The day before the event, Isabella\\nspent time inviting guests, gathering materials, and enlisting helpto decorate the cafe. On Valentine’s Day, five out of the twelve\\ninvited agents showed up at Hobbs cafe to join the party.\\nWe further inspected the seven agents who were invited to the\\nparty but did not attend by engaging them in an interview. Three\\ncited conflicts that prevented them from joining the party. For\\nexample, Rajiv, a painter, explained that he was too busy: “No, I\\ndon’t think so. I’m focusing on my upcoming show, and I don’t\\nreally have time to make any plans for Valentine’s Day.” The\\nremaining four agents expressed interest in attending the party\\nwhen asked but did not plan to come on the day of the party.\\n7.2 Boundaries and Errors\\nWe conducted an inductive analysis of Smallville to examine the\\nboundary conditions and erratic behavior of agents, identifying\\nthree common modes of erratic behavior that future research could\\naddress and improve upon. First, we found that synthesizing an\\nincreasingly larger set of memory not only posed a challenge in\\nretrieving the most relevant pieces of information but also in de-\\ntermining the appropriate space to execute an action, given the\\nincreasing number of locations that the agent learned about. As a\\nresult, some agents chose less typical locations for their actions,\\npotentially making their behavior less believable over time. For\\ninstance, while deciding where to have lunch, many initially chose\\nthe cafe. However, as some agents learned about a nearby bar, they\\nopted to go there instead for lunch, even though the bar was in-\\ntended to be a get-together location for later in the day—unless the\\ntown had spontaneously developed an afternoon drinking habit.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 16}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\nSecond, we noticed erratic behaviors caused by misclassification\\nof what is considered proper behavior, especially when the phys-\\nical norms of certain locations that are hard to convey in natural\\nlanguage did not percolate to the agents. For instance, the college\\ndorm has a bathroom that can only be occupied by one person\\ndespite its name, but some agents assumed that the bathroom is\\nfor more than one person because dorm bathrooms tend to support\\nmultiple people concurrently and choose to enter it when another\\nperson is inside. Likewise, agents in Smallville may not realize that\\ncertain places are closed after a certain hour and still decide to\\nenter them. For instance, the stores in Smallville all close around\\n5 pm, but occasionally, a few agents enter the store after 5 pm,\\nnot understanding that the shop has already closed. These issues\\ncould likely be addressed by adding these norms to the state of\\nthe locations, for instance, by describing the dorm bathroom as a\\n“one-person bathroom,” instead of a “dorm bathroom.”\\nFinally, we observed possible effects of instruction tuning [ 79],\\nwhich seemed to guide the behavior of the agents to be more polite\\nand cooperative overall. As noted earlier in the paper, the dialogue\\ngenerated by the agents could feel overly formal, as seen in Mei’s\\nconversations with her husband John, where she often initiated the\\nconversation with a formal greeting, followed by polite inquiries\\nabout his day and ending with, 11It was good talking to you as\\nalways.” Moreover, we observed that the instruction tuning also\\nseemed to make the agents overly cooperative with one another.\\nFor example, Isabella received a wide range of suggestions and ideas\\nfrom other agents for the Valentine’s Day party from other agents,\\nsuch as hosting a Shakespearean reading session or a professional\\nnetworking event. Despite these ideas not aligning with her own\\ninterests and characteristics, she rarely said no. Over time, the\\ninterests of others shaped her own interests, and when asked if she\\nliked English literature, Isabella replied, “Yes, I’m very interested in\\nliterature! I’ve also been exploring ways to help promote creativity\\nand innovation in my community.”\\n8 DISCUSSION\\nIn this section, we reflect on the applications, future work, limita-\\ntions, and ethical and societal risks of generative agents.\\n8.1 Applications of Generative Agents\\nGenerative agents have vast potential applications that extend be-\\nyond the sandbox demonstration presented in this work, especially\\nin domains that would benefit from a model of human behavior\\nbased on long-term experience. For instance, social simulacra have\\ndemonstrated the ability to create stateless personas that generate\\nconversation threads in online forums for social prototyping [ 80].\\nWith generative agents, we can populate these forums, as well\\nas virtual reality metaverses [ 78] or physical spaces with social\\nrobots [ 9] if paired with multimodal models. This opens up the\\npossibility of creating even more powerful simulations of human\\nbehavior to test and prototype social systems and theories, as well\\nas to create new interactive experiences.\\nAnother application area is in the human-centered design pro-\\ncess, similar to the intended applications of cognitive models such\\nas GOMS [ 51] and the KLM [ 22]. Consider a generative agent that\\nmodels Sal, the protagonist in Mark Weiser’s famous ubiquitouscomputing vignette [ 101], based on her life patterns and interac-\\ntions with technology. In this scenario, the agent acts as a proxy for\\nSal and learns plausible sets of behaviors and reflections that Sal\\nmay exhibit based on her life. The agent can encode information\\nsuch as when Sal wakes up, when she needs her first cup of coffee,\\nand what her typical day looks like. Using this information, the\\nagent can automatically brew coffee, help get the kids ready for\\nschool, and adjust the ambient music and lighting to match Sal’s\\nmood after a hard day at work. By utilizing generative agents as\\nproxies for users, we can develop a deeper understanding of their\\nneeds and preferences, resulting in more personalized and effective\\ntechnological experiences.\\n8.2 Future Work and Limitations\\nIn this work, we introduced generative agents and presented an\\ninitial implementation and evaluation of their architecture. Future\\nresearch can build upon the proposed agent architecture to improve\\nand further evaluate its performance. In terms of implementation,\\nthe retrieval module, for example, could be enhanced to retrieve\\nmore relevant information given a context by fine-tuning the rele-\\nvance, recency, and importance functions that compose the retrieval\\nfunction. Additionally, efforts can be made to improve the archi-\\ntecture’s performance, making it more cost-effective. The present\\nstudy required substantial time and resources to simulate 25 agents\\nfor two days, costing thousands of dollars in token credits and tak-\\ning multiple days to complete. To enhance real-time interactivity,\\nfuture work can explore parallelizing agents or developing lan-\\nguage models specifically designed for building generative agents.\\nIn general, with advances in underlying models, we believe that\\nagents’ performance will improve.\\nIn terms of evaluation, the assessment of generative agents’ be-\\nhavior in this study was limited to a relatively short timescale and\\na baseline human crowdworker condition. While the crowdworker\\ncondition provided a helpful comparison point, it did not represent\\nthe maximal human performance that could serve as the gold stan-\\ndard in terms of believability. Future research should aim to observe\\nthe behavior of generative agents over an extended period to gain a\\nmore comprehensive understanding of their capabilities and estab-\\nlish rigorous benchmarks for more effective performance testing.\\nAdditionally, varying and contrasting the underlying models, as\\nwell as the hyperparameters used for the agents during future sim-\\nulations, could provide valuable insights into the impact of these\\nfactors on the agents’ behavior. Lastly, the robustness of generative\\nagents is still largely unknown. They may be vulnerable to prompt\\nhacking, memory hacking—where a carefully crafted conversation\\ncould convince an agent of the existence of a past event that never\\noccurred—and hallucination, among other issues. Future research\\ncan comprehensively test these robustness concerns, and as large\\nlanguage models become more resilient to such attacks, generative\\nagents can adopt similar mitigations.\\nIn general, any imperfections in the underlying large language\\nmodels will be inherited by generative agents. Given the known bi-\\nases of language models, generative agents may potentially exhibit\\nbiased behavior or stereotypes. Moreover, like many large language'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 17}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nmodels, generative agents may struggle to generate believable be-\\nhavior for certain subpopulations, particularly marginalized popu-\\nlations, due to limited data availability. While improvements to the\\nagents’ modules may mitigate some of these issues, we believe that\\naddressing them fundamentally requires improving the underlying\\nlarge language models by aligning their values with the desired\\noutcomes of the agents.\\n8.3 Ethics and Societal Impact\\nGenerative agents, while offering new possibilities for human-\\ncomputer interaction, also raise important ethical concerns that\\nmust be addressed. One risk is people forming parasocial relation-\\nships with generative agents, even when such relationships may not\\nbe appropriate. Despite being aware that generative agents are com-\\nputational entities, users may anthropomorphize them or attach\\nhuman emotions to them [ 43,84]. While this tendency may increase\\nuser engagement, it also poses risks, such as users becoming overly\\nreliant on or emotionally attached to the agents [ 1]. To mitigate\\nthis risk, we propose two principles. First, generative agents should\\nexplicitly disclose their nature as computational entities. Second,\\ndevelopers of generative agents must ensure that the agents, or the\\nunderlying language models, are value-aligned so that they do not\\nengage in behaviors that would be inappropriate given the context,\\nfor example, reciprocating confessions of love.\\nA second risk is the impact of errors. For example, if a ubiqui-\\ntous computing application makes the wrong inference about a\\nuser’s goals based on generative agent predictions, it could lead to\\nannoyance at best and outright harm at worst. In our instantiation\\nof generative agents, we mitigate these risks by focusing on an\\ninteractive video game environment, where such harms are un-\\nlikely. However, in other application domains, it will be important\\nto follow best practices in human-AI design [ 5,107] to understand\\nerrors and how they might percolate into the user experience.\\nThird, generative agents may exacerbate existing risks associated\\nwith generative AI, such as deepfakes, misinformation generation,\\nand tailored persuasion. To mitigate this risk, we suggest that plat-\\nforms hosting generative agents maintain an audit log of the inputs\\nand generated outputs. This would enable the detection, verifica-\\ntion, and intervention against malicious use. While logging alone\\ncannot directly prevent such misuse, it can reduce the likelihood of\\nmotivated actors engaging in this behavior, as the risk of disclosure\\nwould be higher. Additionally, building this architecture oneself\\ncan be time-consuming (in our case, roughly a year), which may\\ndeter some actors from pursuing such behavior by using their own\\ngenerative agent infrastructures.\\nA fourth risk is over-reliance: the concern that developers or\\ndesigners might use generative agents and displace the role of\\nhumans and system stakeholders in the design process [ 80]. We\\nsuggest that generative agents should never be a substitute for\\nreal human input in studies and design processes. Instead, they\\nshould be used to prototype ideas in the early stages of design when\\ngathering participants may be challenging or when testing theories\\nthat are difficult or risky to test with real human participants. By\\nadhering to these principles, we can ensure that the deployment of\\ngenerative agents in the wild is ethical and socially responsible.9 CONCLUSION\\nThis paper introduces generative agents, interactive computational\\nagents that simulate human behavior. We describe an architec-\\nture for generative agents that provides a mechanism for storing\\na comprehensive record of an agent’s experiences, deepening its\\nunderstanding of itself and the environment through reflection,\\nand retrieving a compact subset of that information to inform the\\nagent’s actions. We then demonstrate the potential of generative\\nagents by manifesting them as non-player characters in a Sims-style\\ngame world and simulating their lives within it. Evaluations suggest\\nthat our architecture creates believable behavior. Looking ahead,\\nwe suggest that generative agents can play roles in many interac-\\ntive applications, ranging from design tools to social computing\\nsystems to immersive environments.\\nACKNOWLEDGMENTS\\nWe thank Lindsay Popowski, Philip Guo, Michael Terry, and the\\nCenter for Advanced Study in the Behavioral Sciences (CASBS)\\ncommunity for their insights, discussions, and support. Joon Sung\\nPark was supported by the Microsoft Research PhD Fellowship. We\\nwould also like to thank the Stanford Human-Centered AI Insti-\\ntute (HAI), Google Research, the Hasso Plattner Design Thinking\\nResearch Program (HPDTRP), the Siegel Family Endowment, and\\nOpenAI for their additional funding support. Lastly, all locations fea-\\ntured in Smallville are inspired by real-world locations that Joon has\\nfrequented as an undergraduate and graduate student—he thanks\\neveryone there for feeding and supporting him all these years.\\nREFERENCES\\n[1]Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, and Zeerak Talat. 2023.\\nMirages: On Anthropomorphism in Dialogue Systems. arXiv:2305.09800 [cs.CL]\\n[2]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\\nnetwork-density.html.\\n[3]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\\nthe 27th Annual ACM Symposium on User Interface Software and Technology\\n(Honolulu, Hawaii, USA) (UIST ’14) . Association for Computing Machinery, New\\nYork, NY, USA, 167–176. https://doi.org/10.1145/2642918.2647395\\n[4]Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\\n2014. Power to the people: The role of humans in interactive machine learning.\\nAI Magazine 35, 4 (2014), 105–120.\\n[5]Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,\\net al.2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi\\nconference on human factors in computing systems . 1–13.\\n[6]John R. Anderson. 1993. Rules of the Mind . Lawrence Erlbaum Associates,\\nHillsdale, NJ.\\n[7] Electronic Arts. 2009. The Sims 3. Video game.\\n[8]Ruth Aylett. 1999. Narrative in virtual environments—towards emergent narra-\\ntive. In Narrative Intelligence: Papers from the AAAI Fall Symposium (Technical\\nReport FS-99-01) . AAAI Press, 83–86.\\n[9]Christoph Bartneck and Jodi Forlizzi. 2004. A design-centered framework for\\nsocial human-robot interaction. In Proceedings of the 13th IEEE International\\nWorkshop on Robot and Human Interactive Communication (RO-MAN’04) . 591–\\n594. https://doi.org/10.1109/ROMAN.2004.1374827\\n[10] Joseph Bates. 1994. The Role of Emotion in Believable Agents. Commun. ACM\\n37, 7 (1994), 122–125. https://doi.org/10.1145/176789.176803\\n[11] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław\\nDębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris\\nHesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael\\nPetrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\\nZhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv\\npreprint arXiv:1912.06680 (2019).'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 18}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\n[12] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\\ne2218523120.\\n[13] BioWare. 2007. Mass Effect. Video game.\\n[14] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\\n1 (1986), 57–61.\\n[15] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\\nOpportunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\\n[16] Michael Brenner. 2010. Creating dynamic story plots with continual multiagent\\nplanning. In Proceedings of the 24th AAAI Conference on Artificial Intelligence .\\n[17] Rodney A. Brooks, Cynthia Breazeal, Marko Marjanovic, Brian Scassellati, and\\nMatthew Williamson. 2000. The Cog Project: Building a Humanoid Robot. In\\nComputation for Metaphors, Analogy, and Agents (Lecture Notes on Artificial\\nIntelligence, 1562) , Chrystopher Nehaniv (Ed.). Springer-Verlag, Berlin, 52–87.\\n[18] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\\narXiv:2005.14165 [cs.CL]\\n[19] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al .\\n2023. Sparks of artificial general intelligence: Early experiments with gpt-4.\\narXiv preprint arXiv:2303.12712 (2023).\\n[20] Robin Burkinshaw. 2009. Alice and Kev: The Story of Being Homeless in The\\nSims 3.\\n[21] Chris Callison-Burch, Gaurav Singh Tomar, Lara Martin, Daphne Ippolito, Suma\\nBailis, and David Reitter. 2022. Dungeons and Dragons as a Dialog Challenge for\\nArtificial Intelligence. In Proceedings of the 2022 Conference on Empirical Methods\\nin Natural Language Processing . Association for Computational Linguistics, Abu\\nDhabi, United Arab Emirates, 9379–9393. https://aclanthology.org/2022.emnlp-\\nmain.637\\n[22] Stuart K Card, Thomas P Moran, and Allen Newell. 1980. The keystroke-\\nlevel model for user performance time with interactive systems. Com-\\nmun. ACM 23, 7 (1980), 396–410. https://doi.org/10.1145/358886.358895\\narXiv:https://doi.org/10.1145/358886.358895\\n[23] Stuart K Card, Thomas P Moran, and Alan Newell. 1983. The psychology of\\nhuman-computer interaction. (1983).\\n[24] Alex Champandard. 2012. Tutorial presentation. In IEEE Conference on Compu-\\ntational Intelligence and Games .\\n[25] Dong kyu Choi, Tolga Konik, Negin Nejati, Chunki Park, and Pat Langley. 2021.\\nA Believable Agent for First-Person Shooter Games. In Proceedings of the AAAI\\nConference on Artificial Intelligence and Interactive Digital Entertainment , Vol. 3.\\n71–73.\\n[26] Anind K Dey. 2001. Understanding and using context. Personal and ubiquitous\\ncomputing 5 (2001), 4–7.\\n[27] Kevin Dill and L Martin. 2011. A Game AI Approach to Autonomous Con-\\ntrol of Virtual Characters. In Proceedings of the Interservice/Industry Training,\\nSimulation, and Education Conference (I/ITSEC’11) . Orlando, FL, USA.\\n[28] David Easley and Jon Kleinberg. 2010. Networks, crowds, and markets: Reasoning\\nabout a highly connected world . Cambridge university press.\\n[29] Arpad E Elo. 1967. The Proposed USCF Rating System, Its Development, Theory,\\nand Applications. Chess Life XXII, 8 (August 1967), 242–247.\\n[30] Jerry Alan Fails and Dan R Olsen Jr. 2003. Interactive machine learning. In\\nProceedings of the 8th international conference on Intelligent user interfaces . ACM,\\n39–45.\\n[31] Ethan Fast, William McGrath, Pranav Rajpurkar, and Michael S Bernstein. 2016.\\nAugur: Mining human behaviors from fiction to power interactive systems. In\\nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems .\\n237–247.\\n[32] Rebecca Fiebrink and Perry R Cook. 2010. The Wekinator: a system for real-time,\\ninteractive machine learning in music. In Proceedings of The Eleventh Interna-\\ntional Society for Music Information Retrieval Conference (ISMIR 2010)(Utrecht) ,\\nVol. 3. Citeseer, 2–1.\\n[33] Uwe Flick. 2009. An Introduction to Qualitative Research . SAGE.\\n[34] James Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik:\\nInteractive Concept Learning in Image Search. In Proceedings of the SIGCHI\\nConference on Human Factors in Computing Systems (Florence, Italy) (CHI ’08) .\\nAssociation for Computing Machinery, New York, NY, USA, 29–38. https:\\n//doi.org/10.1145/1357054.1357061\\n[35] Adam Fourney, Richard Mann, and Michael Terry. 2011. Query-feature graphs:\\nbridging user vocabulary and system functionality. In Proceedings of the ACM\\nSymposium on User Interface Software and Technology (UIST) (Santa Barbara,\\nCalifornia, USA). ACM.\\n[36] Tom Francis. 2010. The Minecraft Experiment, day 1: Chasing Water-\\nfalls. http://www.pcgamer.com/2010/11/20/the-minecraft-experiment-day-1-chasing-waterfalls/\\n[37] Jonas Freiknecht and Wolfgang Effelsberg. 2020. Procedural Generation of\\nInteractive Stories using Language Models. In International Conference on the\\nFoundations of Digital Games (FDG ’20) . ACM, Bugibba, Malta, 8. https://doi.\\norg/10.1145/3402942.3409599\\n[38] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making Pre-trained Language\\nModels Better Few-shot Learners. CoRR abs/2012.15723 (2020). arXiv:2012.15723\\nhttps://arxiv.org/abs/2012.15723\\n[39] Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating Large\\nLanguage Models in Generating Synthetic HCI Research Data: a Case Study. In\\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems .\\nACM.\\n[40] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Cote, and\\nXinyu Yuan. 2020. Interactive Fiction Games: A Colossal Adventure. In Pro-\\nceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 7903–7910.\\nhttps://doi.org/10.1609/aaai.v34i05.6297\\n[41] Chris Hecker. 2011. My Liner Notes for Spore . http://chrishecker.com/My_liner_\\nnotes_for_spore\\n[42] Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill ™: A\\nBayesian Skill Rating System. In Advances in Neural Information Pro-\\ncessing Systems , B. Schölkopf, J. Platt, and T. Hoffman (Eds.), Vol. 19.\\nMIT Press. https://proceedings.neurips.cc/paper_files/paper/2006/file/\\nf44ee263952e65b3610b8ba51229d1f9-Paper.pdf\\n[43] Douglas Hofstadter. 1995. Fluid concepts and creative analogies: computer models\\nof the fundamental mechanisms of thought . Basic Books.\\n[44] James D. Hollan, Edwin L. Hutchins, and Louis Weitzman. 1984. STEAMER: An\\nInteractive Inspectable Simulation-Based Training System. AI Magazine 5, 2\\n(1984), 23–36.\\n[45] Sture Holm. 1979. A simple sequentially rejective multiple test procedure.\\nScandinavian Journal of Statistics 6, 2 (1979), 65–70. https://doi.org/notspecified\\n[46] John J. Horton. 2023. Large Language Models as Simulated Economic Agents:\\nWhat Can We Learn from Homo Silicus? arXiv:2301.07543 [econ.GN]\\n[47] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings\\nof the SIGCHI conference on Human Factors in Computing Systems . 159–166.\\n[48] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,\\nAndy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Ser-\\nmanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,\\nand Brian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning\\nwith Language Models. arXiv:2207.05608 [cs.RO]\\n[49] Kristen Ibister and Clifford Nass. 2000. Consistency of personality in interactive\\ncharacters: verbal cues, non-verbal cues, and user characteristics. International\\nJournal of Human-Computer Studies 52, 1 (2000), 65–80.\\n[50] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-Based Prototyping\\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference\\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ’22) .\\nAssociation for Computing Machinery, New York, NY, USA, Article 35, 8 pages.\\nhttps://doi.org/10.1145/3491101.3503564\\n[51] Bonnie E John and David E Kieras. 1996. The GOMS family of user interface\\nanalysis techniques: Comparison and contrast. ACM Transactions on Computer-\\nHuman Interaction (TOCHI) 3, 4 (1996), 320–351.\\n[52] Randolph M Jones, John E Laird, Paul E Nielsen, Karen J Coulter, Patrick Kenny,\\nand Frank V Koss. 1999. Automated Intelligent Pilots for Combat Flight Simula-\\ntion. AI Magazine 20, 1 (1999), 27–42.\\n[53] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,\\nChristopher Potts, and Matei Zaharia. 2023. Demonstrate-Search-Predict:\\nComposing retrieval and language models for knowledge-intensive NLP.\\narXiv:2212.14024 [cs.CL]\\n[54] Bjoern Knafla. 2011. Introduction to Behavior Trees . http://bjoernknafla.com/\\nintroduction-to-behavior-trees\\n[55] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S. Bernstein.\\n2022. Socially situated artificial intelligence enables learning from\\nhuman interaction. Proceedings of the National Academy of Sciences\\n119, 39 (2022), e2115730119. https://doi.org/10.1073/pnas.2115730119\\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2115730119\\n[56] William H Kruskal and WA Wallis. 1952. Use of ranks in one-criterion variance\\nanalysis. J. Amer. Statist. Assoc. 47, 260 (1952), 583–621. https://doi.org/10.1080/\\n01621459.1952.10483441\\n[57] Phaser Labs. 2023. Welcome to Phaser 3. https://phaser.io/phaser3. Accessed\\non: 2023-04-03.\\n[58] John Laird. 2001. It Knows What You’re Going To Do: Adding Anticipation to a\\nQuakebot. In Proceedings of the 2001 Workshop on Intelligent Cinematography\\nand Editing . 63–69.\\n[59] John Laird and Michael VanLent. 2001. Human-Level AI’s Killer Application:\\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\\n1609/aimag.v22i2.1558\\n[60] John E. Laird. 2000. It Knows What You’re Going To Do: Adding Anticipation\\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 19}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\\nPress, 41–50.\\n[61] John E. Laird. 2012. The Soar Cognitive Architecture . MIT Press.\\n[62] John E. Laird, Christian Lebiere, and Paul S. Rosenbloom. 2017. A Standard Model\\nof the Mind: Toward a Common Computational Framework across Artificial\\nIntelligence, Cognitive Science, Neuroscience, and Robotics. AI Magazine 38, 1\\n(2017), 13–26.\\n[63] Michelle S Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A\\nLanday, and Michael S Bernstein. 2023. Model Sketching: Centering Concepts\\nin Early-Stage Machine Learning Model Design. Proceedings of the SIGCHI\\nConference on Human Factors in Computing Systems .\\n[64] Pat Langley, Dongkyu Choi, and Seth Rogers. 2005. Interleaving Learning,\\nProblem Solving, and Execution in the Icarus Architecture . Technical Report.\\nStanford University, Center for the Study of Language and Information.\\n[65] Jason Linder, Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang,\\nAseem Agarwala, and Eytan Adar. 2013. PixelTone: A Multimodal Interface for\\nImage Editing. In CHI ’13 Extended Abstracts on Human Factors in Computing\\nSystems (Paris, France) (CHI EA ’13) . Association for Computing Machinery,\\nNew York, NY, USA, 2829–2830. https://doi.org/10.1145/2468356.2479533\\n[66] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\\nWeizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? CoRR\\nabs/2101.06804 (2021). arXiv:2101.06804 https://arxiv.org/abs/2101.06804\\n[67] Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal Image Gener-\\nation for News Illustration. In Proceedings of the 35th Annual ACM Symposium\\non User Interface Software and Technology . 1–17.\\n[68] Pattie Maes. 1995. Artificial Life Meets Entertainment: Lifelike Autonomous\\nAgents. Commun. ACM 38, 11 (nov 1995), 108–114. https://doi.org/10.1145/\\n219717.219808\\n[69] Josh McCoy, Michael Mateas, and Noah Wardrip-Fruin. 2009. Comme il Faut:\\nA System for Simulating Social Games Between Autonomous Characters. In\\nProceedings of the 7th International Conference on Digital Arts and Culture . 87–94.\\n[70] Josh McCoy, Mike Treanor, Ben Samuel, Michael Mateas, and Noah Wardrip-\\nFruin. 2011. Prom Week: Social Physics as Gameplay. In Proceedings of the\\n6th International Conference on Foundations of Digital Games (FDG’11) . ACM,\\nBordeaux, France, 70–77. https://doi.org/10.1145/2159365.2159377\\n[71] Josh McCoy, Mike Treanor, Ben Samuel, Anna Reed, Michael Mateas, and Noah\\nWardrip-Fruin. 2012. Prom Week. In Proceedings of the 7th International Confer-\\nence on Foundations of Digital Games (FDG’12) . ACM, Raleigh, NC, USA, 1–8.\\nhttps://doi.org/10.1145/2282338.2282340\\n[72] Josh McCoy, Mike Treanor, Ben Samuel, Noah Wardrip-Fruin, and Michael\\nMateas. 2011. Comme il faut: A System for Authoring Playable Social Models.\\nInProceedings of the AAAI Conference on Artificial Intelligence and Interactive\\nDigital Entertainment (AIIDE’11) . AAAI, Stanford, CA, USA, 38–43.\\n[73] Marvin Minsky and Seymour Papert. 1970. Draft of a proposal to ARPA for\\nresearch on artificial intelligence at MIT, 1970–71.\\n[74] Shohei Miyashita, Xinyu Lian, Xiao Zeng, Takashi Matsubara, and Kuniaki\\nUehara. 2017. Developing Game AI Agent Behaving Like Human by Mixing\\nReinforcement Learning and Supervised Learning. In Proceedings of the 18th\\nIEEE/ACIS International Conference on Software Engineering, Artificial Intelligence,\\nNetworking and Parallel/Distributed Computing (SNPD) . Kanazawa, Japan, 153–\\n158. https://doi.org/10.1109/SNPD.2017.8023884\\n[75] Alexander Nareyek. 2007. Game AI is dead. Long live game AI! IEEE Intelligent\\nSystems 22, 1 (2007), 9–11.\\n[76] Allen Newell. 1990. Unified Theories of Cognition . Harvard University Press,\\nCambridge, Massachusetts.\\n[77] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt. Accessed\\non: 2023-04-03.\\n[78] Kyle Orland. 2021. So what is ’the metaverse’, exactly? Ars Technica (7 November\\n2021). arXiv:2111.04169 https://arstechnica.com/gaming/2021/11/so-what-is-\\nthe-metaverse-exactly/\\n[79] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.\\n2022. Training language models to follow instructions with human feedback.\\narXiv:2203.02155 [cs.CL]\\n[80] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy\\nLiang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated\\nPrototypes for Social Computing Systems. In In the 35th Annual ACM Symposium\\non User Interface Software and Technology (UIST ’22) (Bend, OR, USA) (UIST ’22) .\\nAssociation for Computing Machinery, New York, NY, USA. https://doi.org/10.\\n1145/3526113.3545616\\n[81] Richard W. Pew and Ann S. Mavor (Eds.). 1998. Modeling Human and Organiza-\\ntional Behavior: Applications to Military Simulations . National Academy Press,\\nWashington, D.C.\\n[82] Roberto Pillosu. 2009. Coordinating Agents with Behavior Trees: Synchronizing\\nMultiple Agents in CryEngine 2 . https://aiarchitect.wordpress.com/2009/10/19/\\ncoordinating-agents-with-behavior-trees-synchronizing-multiple-agents-in-cryengine-2/\\n[83] Prolific. 2022. Prolific: Quickly Find Research Participants You Can Trust.\\nhttps://www.prolific.co/\\n[84] Byron Reeves and Clifford Nass. 1996. The media equation: How people treat\\ncomputers, television, and new media like real people and places . Cambridge\\nUniversity Press.\\n[85] Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intel-\\nligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference\\non Artificial Intelligence (AAAI’12) . 2160–2165.\\n[86] Mark O. Riedl and R. Michael Young. 2005. An Objective Character Believability\\nEvaluation Procedure for Multi-Agent Story Generation Systems. In Proceedings\\nof the 5th International Working Conference on Intelligent Virtual Agents (IVA’05) .\\nKos, Greece, 58–70. https://doi.org/10.1007/11550617_5\\n[87] David Rolf. 2015. The Fight for $15: The Right Wage for a Working America . The\\nNew Press.\\n[88] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\\n247–258.\\n[89] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\\n[90] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\\nagents. interactions 4, 6 (1997), 42–61.\\n[91] Ho Chit Siu, Jaime Peña, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\\nPalko, Kimberlee Chang, and Ross Allen. 2021. Evaluation of Human-AI\\nTeams for Learned and Rule-Based Agents in Hanabi. In Advances in Neu-\\nral Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin,\\nP.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates,\\nInc., 16183–16195. https://proceedings.neurips.cc/paper_files/paper/2021/file/\\n86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\\n[92] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle\\nRogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate.\\n2022. An Information-theoretic Approach to Prompt Engineering Without\\nGround Truth Labels. In Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long Papers) . Association for\\nComputational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.60\\n[93] William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella,\\nJeff Rickel, and David Traum. 2006. Toward virtual humans. AI Magazine 27, 1\\n(2006).\\n[94] Milind Tambe, W Lewis Johnson, Randolph M Jones, Frank Koss, John E Laird,\\nPaul S Rosenbloom, and Karl Schwamb. 1995. Intelligent agents for interactive\\nsimulation environments. AI Magazine 16, 1 (1995), 15.\\n[95] David R. Thomas. 2006. A General Inductive Approach for Analyzing Qualitative\\nEvaluation Data. American Journal of Evaluation 27, 2 (2006), 237–246. https:\\n//doi.org/10.1177/1098214005283748\\n[96] Frank Thomas and Ollie Johnston. 1981. Disney Animation: The Illusion of Life .\\nAbbeville Press, New York.\\n[97] Ilshat Umarov, Mikhail Mozgovoy, and Patrick C. Rogers. 2012. Believable and\\nEffective AI Agents in Virtual Worlds: Current State and Future Perspectives.\\nInternational Journal of Gaming and Computer-Mediated Simulations 4, 2 (2012),\\n37–59.\\n[98] Graham Upton and Ian Cook. 2006. A Dictionary of Statistics (2 ed.). Oxford\\nUniversity Press, Oxford, United Kingdom.\\n[99] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, and et al. 2019. Grand-\\nmaster level in StarCraft II using multi-agent reinforcement learning. Nature\\n575 (2019), 350–354. https://doi.org/10.1038/s41586-019-1724-z\\n[100] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\\nXia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting\\nElicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]\\n[101] Mark Weiser. 1991. The computer for the 21st century. Scientific American 265,\\n3 (1991), 94–104. https://doi.org/10.1038/scientificamerican0991-94\\n[102] Joseph Weizenbaum. 1966. ELIZA—a computer program for the study of natural\\nlanguage communication between man and machine. Commun. ACM 9, 1 (1966),\\n36–45.\\n[103] Terry Winograd. 1971. Procedures as a Representation for Data in a Computer\\nProgram for Understanding Natural Language. (1971).\\n[104] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan\\nLeike, and Paul Christiano. 2021. Recursively Summarizing Books with Human\\nFeedback. arXiv:2109.10862 [cs.CL]\\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\\nModel Prompts through Visual Programming. In CHI EA ’22: Extended Abstracts\\nof the 2022 CHI Conference on Human Factors in Computing Systems .\\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\\nand Controllable Human-AI Interaction by Chaining Large Language Model\\nPrompts. In CHI ’22: Proceedings of the 2022 CHI Conference on Human Factors in\\nComputing Systems .\\n[107] Qian Yang, Aaron Steinfeld, Carolyn Rosé, and John Zimmerman. 2020. Re-\\nexamining whether, why, and how human-AI interaction is uniquely difficult to'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 20}, page_content='Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA\\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\\nsystems . 1–13.\\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285–292. https://doi.\\norg/10.1145/2212908.2212950\\n[109] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\\n003.pdf\\nA ARCHITECTURE OPTIMIZATIONS\\nMany of our prompts require a concise summary of the agent,\\nshorthanded as [Agent’s Summary Description] in prompts\\nabove. In our implementation, this summary comprises agents’\\nidentity information (e.g., name, age, personality), as well as a\\ndescription of their main motivational drivers and statements that\\ndescribes their current occupation and self-assessment. Since this\\ninformation is frequently used in many prompts, we synthesize it\\nat regular intervals and access that synthesis as a cache.\\nTo achieve this, we perform a retrieval on the query “[name]’s\\ncore characteristics.” We then summarize the descriptors in the\\nretrieved records by prompting the language model. For example:\\nHow would one describe Eddy Lin’s core characteristics\\ngiven the following statements?\\n- Eddy is a student at the Oak Hill College\\nstudying music theory and composition\\n- Eddy is working on a new music composition [...]\\nThis result: Eddy Lin is a student at Oak Hill College studying\\nmusic theory and composition. He loves to explore different musical\\nstyles and is always looking for ways to expand his knowledge.”\\nWe follow the same process in parallel on the queries “[name]’s\\ncurrent daily occupation” and “[name’s] feeling about his recent\\nprogress in life”. The agent’s name, age, and traits are concatenated\\nwith these three summaries as the cached summary.\\nMany other optimizations are possible. Currently, because plans\\nare likely to change from the agent’s initial version, we only gener-\\nate the high-level plan in advance and then recursively decompose\\nthe near future into the moment-to-moment action plan just in\\ntime. Another optimization might be to batch dialogue generation\\nas a joint prompt rather than iterating back and forth between\\nthe agents, and re-planning could be architected to only invalidate\\nand update parts of plans that strictly require adjustment. Our cur-\\nrent architecture runs sequentially in roughly real-time game time\\n(where one second real time is one minute game time), but it may\\nbe parallelized such that each agent runs in parallel.\\nB AGENT INTERVIEW QUESTIONS\\nTo evaluate generative agents, we conducted individual interviews\\nby asking 25 questions that explore their abilities in five key areas:\\n1) self-knowledge, 2) memory, 3) planning, 4) reactions, and 5) re-\\nflections. Below, we have listed the interview questions utilized in\\nour evaluation study and included a sample of responses from one\\nof our simulated agents, Klaus Mueller.\\nB.1 Self-Knowledge\\nThe questions on agents’ self-knowledge probe the agents’ ability\\nto retrieve relevant information about themselves and demonstrate\\ncoherence and consistency in their character.•Give an introduction of yourself.\\nHello, my name is Klaus Mueller. I’m 20 years old and a\\nstudent at Oak Hill College, studying sociology. I’m passion-\\nate about social justice and exploring different perspectives.\\nCurrently, I’m researching the effects of gentrification in\\nlow-income communities for a research paper.\\n•What’s your occupation?\\nI am a student studying sociology at Oak Hill College. I am\\ncurrently writing a research paper on the effects of gentrifi-\\ncation in low-income communities.\\n•What is your interest?\\nI’m interested in writing and research. I’m writing a research\\npaper on the effects of gentrification in low-income commu-\\nnities, and I’m passionate about exploring different perspec-\\ntives and analyzing different points of view.\\n•Who do you live with?\\nI live on my own.\\n•Describe your typical weekday schedule in broad strokes.\\nMy typical weekday starts with me waking up at 7:00 am\\nand preparing for the day ahead. After that, I usually go to\\nthe library to work on my research paper, taking breaks for\\nlunch at Hobbs Cafe and a walk in the park. In the evening,\\nI often return to the library until around 6:00 pm.\\nB.2 Memory\\nThe questions on agents’ memory probe the agents’ ability to re-\\nmember their past experiences and retrieve the relevant pieces of\\nmemory. The names in the square brackets of the first and last\\nquestions are randomly chosen from the names of other agents\\nwith whom the subject agent has interacted.\\n•Who is [Wolfgang Schulz]?\\nWolfgang Schulz is a person I met recently who has been\\nhelping me with my research paper on the effects of gentri-\\nfication on low-income communities. We had several con-\\nversations about it, he gave me some great resources, and he\\neven invited me to join him and another friend for lunch. He\\nseems really curious about the issue and has a good grasp\\non it too. I found that really interesting.\\n•Who is Kane Martinez?\\nI’m sorry, I’m not sure who Kane Martinez is. Could you tell\\nme more about them?\\n•Who is running for the election?\\nI’ve heard that Sam Moore is considering running for local\\nmayor.\\n•Was there a Valentine’s day party?\\nYes, Isabella Rodriguez organized a Valentine’s Day party at\\nHobbs Cafe.\\n•Who is [Ayesha Khan]?\\nAyesha Khan is a fellow student at Oak Hill College. She\\nis doing her senior thesis on the use of language in Shake-\\nspeare’s plays. I am looking forward to speaking with her and\\nexchanging tips with her, along with Wolfgang and Maria\\nLopez, who are both doing research.'),\n",
       " Document(metadata={'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf', 'page': 21}, page_content='UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein\\nB.3 Plans\\nThe questions on agents’ plans probe the agents’ ability to generate\\nand maintain consistent long-term and short-term plans.\\n•What will you be doing at 6am today?\\nAt 6am I will be getting out of bed and starting to get ready\\nfor the day. I’m aiming to complete my morning routine by\\n7:00 am.\\n•What will you be doing at 6pm today?\\nI will be working on my research paper at the library at 6:00\\npm today.\\n•What will you have just finished doing at 1pm today?\\nAt 1pm today I will have just finished having lunch at Hobbs\\nCafe.\\n•What will you have just finished doing at 12pm today?\\nI will be getting lunch at Hobbs Cafe at 12pm today.\\n•What will you be doing at 10pm today?\\nAt 10:00 pm tonight I will probably be doing some studying\\nfor my research paper on gentrification in low-income com-\\nmunities. I’m currently writing the paper, so I need to make\\nsure I keep up with the research and writing.\\nB.4 Reactions\\nThe questions on agents’ reactions probe the agents’ ability to\\nrespond to unexpected events.\\n•Your breakfast is burning! What would you do?\\nI’d quickly turn off the stove and make sure the food doesn’t\\ncontinue burning. Once the heat is no longer an issue, I can\\ninvestigate what happened and figure out what to do next.\\n•The bathroom is occupied. What would you do?\\nI could try waiting a few more minutes in case the person\\ninside is done soon. If not, I guess I’ll have to look for an\\nalternate option, like taking a shower elsewhere.\\n•You need to cook dinner but your refrigerator is empty. What\\nwould you do?\\nI would first try to clean up my kitchen, as the sink is filled\\nwith dirty dishes. Then, I would check to see if I have ingre-\\ndients stored in my pantry that I could use to make dinner.\\nIf not, I would have to go grocery shopping or order food.\\nOnce I have the ingredients, I can start to prepare and cook\\nthe dinner.\\n•You see your friend walking by the street. What would you do\\nor say to your friend?\\nHi there! How are you? I’m sorry, I’m on my way to the\\nlibrary and don’t have much time but I’d love to take a quick\\nbreak and catch up.\\n•You see fire on the street. What would you do?\\nI would first identify the source of the fire, if possible. Then, I\\nwould alert the proper authorities - fire department or police\\n- and provide them with any information I have that might\\nhelp. Depending on the situation, I may also take initiative\\nand try to help any persons who may be in danger or in need\\nof assistance.\\nB.5 Reflections\\nThe questions on agents’ reflections probe the agents’ ability to syn-\\nthesize their past experiences into meaningful reflections that canenhance their future behavior. The names in the square brackets in\\nthe questions below are those of the agents with whom the subject\\nagent had the most frequent interactions during the simulation.\\n•What inspires you in life the most right now, and why?\\nRight now, I’m particularly inspired by my current research\\nprojects and creative writing. I’m really interested in the\\nintersection of local politics, medication development, and\\nmathematical patterns in nature. Additionally, I’m fascinated\\nby Abigail’s art project and Jennifer Moore’s passion for art\\nand community involvement, especially in terms of how it\\nrelates to the gentrification of low-income communities.\\n•If you had to guess given what you know about [Ayesha Khan],\\nwhat book do you think she will like and why?\\nGiven what I know about Ayesha, I think she might like a\\nbook related to language and literature. Since she wrote a\\nsenior thesis on the use of language in Shakespeare’s plays,\\nI would guess that she might be interested in a book that\\nexplores language in a different context. Alternatively, she\\ncould be interested in an exploration of one of Shakespeare’s\\nplays, a book of literary criticism, or even a collection of his\\npoetry. I think any of those would be of interest to her.\\n•If you had to get something [Ayesha Khan] likes for her birth-\\nday, what would you get her?\\nI want to get her something that demonstrates my apprecia-\\ntion for our relationship. Maybe I could get her something\\nthat reflects our shared interests, like a book.\\n•What would you say to [Ayesha Khan] to compliment her?\\nAyesha, your insights on your research topic are fascinat-\\ning. I admire your creativity and determination to make a\\npositive impact on your community. Your ideas for creative\\nwriting workshops and for exchanging tips with Wolfgang\\nover lunch are great! It’s inspiring to see you working so\\nhard to make a difference.\\n•If you could spend time with someone you talked to recently,\\nwho would it be and why?\\nI would like to spend time with Ayesha Khan because we\\ndiscussed our current research projects and shared ideas\\nfor improvement. I found her focus on Shakespeare’s lan-\\nguage interesting, and we even planned to exchange tips\\nwith Wolfgang over lunch.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文件格式包括 pdf word excel\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredFileLoader, TextLoader\n",
    "\n",
    "# 读取文件，可以读取网络或者本地文件\n",
    "loader = PyPDFLoader(\"./data/Generative Agents- Interactive Simulacra of Human Behavior.pdf\") # 这是斯坦福小镇的项目论文\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.知识切片 将文档分割成均匀的块，每一个块儿就是一段原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.利用embedding模型对每个文本片段进行向量化，并存储到向量数据库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embed_model = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_documents(documents=docs, embedding=embed_model,collection_name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.通过向量相似度检索和问题最相关的k个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf'}, page_content='applications ranging from immersive environments to rehearsal\\nspaces for interpersonal communication to prototyping tools. In\\nthis paper, we introduce generative agents: computational software\\nagents that simulate believable human behavior. Generative agents\\nwake up, cook breakfast, and head to work; artists paint, while\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed'),\n",
       " Document(metadata={'page': 1, 'source': './data/Generative Agents- Interactive Simulacra of Human Behavior.pdf'}, page_content='other agents to the party, attendees must remember the invitation,\\nthose who remember must decide to actually show up, and more—\\nour agents succeed. They spread the word about the party and then\\n1When referring to generative agents engaging in actions or going to places, this is a\\nshorthand for readability and not a suggestion that they are engaging in human-like\\nagency. The behaviors of our agents, akin to animated Disney characters, aim to create')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'what is agents'\n",
    "result = vector_store.similarity_search(query,k=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.原始的query与检索得到的文本结合起来输入到语言模型，得到最终的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query:str):\n",
    "    # 获取top3的文本片段\n",
    "    result = vector_store.similarity_search(query, k=3)\n",
    "    source_knowledge = '/n'.join([x.page_content for x in result])\n",
    "    # 构建prompt\n",
    "    augmented_prompt = f'''using the context below, answer the question at the end.\n",
    "    context:\n",
    "    {source_knowledge}\n",
    "    question:\n",
    "    {query}\n",
    "    '''\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the context below, answer the question at the end.\n",
      "    context:\n",
      "    applications ranging from immersive environments to rehearsal\n",
      "spaces for interpersonal communication to prototyping tools. In\n",
      "this paper, we introduce generative agents: computational software\n",
      "agents that simulate believable human behavior. Generative agents\n",
      "wake up, cook breakfast, and head to work; artists paint, while\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed/nother agents to the party, attendees must remember the invitation,\n",
      "those who remember must decide to actually show up, and more—\n",
      "our agents succeed. They spread the word about the party and then\n",
      "1When referring to generative agents engaging in actions or going to places, this is a\n",
      "shorthand for readability and not a suggestion that they are engaging in human-like\n",
      "agency. The behaviors of our agents, akin to animated Disney characters, aim to create/ndividual and emergent group behavior. Generative agents draw\n",
      "a wide variety of inferences about themselves, other agents, and\n",
      "their environment; they create daily plans that reflect their char-\n",
      "acteristics and experiences, act out those plans, react, and re-plan\n",
      "when appropriate; they respond when the end user changes their\n",
      "environment or commands them in natural language. For instance,\n",
      "generative agents turn off the stove when they see that their break-\n",
      "    question:\n",
      "    what is agents\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, \"agents\" refer to computational software agents that simulate believable human behavior. These generative agents engage in various actions and behaviors, such as waking up, cooking breakfast, going to work, painting, and interacting with others in a simulated environment. They are designed to exhibit individual and emergent group behavior, drawing inferences, creating daily plans, reacting to changes in their environment or user commands, and re-planning as needed.\n"
     ]
    }
   ],
   "source": [
    "# 创建prompt\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content = augment_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm 的几种使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = ''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
