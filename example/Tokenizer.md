## Tokenizer
- 作用：将文本序列转化为数字(token)序列，作为transformer的输入
- 分词粒度：word；character；subword
### Word Tokenizer
按照词进行分词，如："I love you" -> ["I", "love", "you"]
优点：简单，容易理解，结果便于理解模型输出
缺点：每个word分配一个id，所需的vocabulary根据语料大小而不同，会将意思一致的词分成两个不同的id
### Character Tokenizer
按照字符进行分词，如："I love you" -> ["I", " ", "l", "o", "v", "e", " ", "y", "o", "u"]
优点：vocabulary相对小的多，适合中文
缺点：对于英语来说，分词后的每个字符是毫无意义的，且输入的长度会变长
### Subword Tokenizer
按照词的子词进行分词，常用于英语，如‘today is sunday’会分割为[to，day，is，sun，day]
优点：vocabulary的大小固定，能保持语意，适合多语言
常见方法：BPE，Byte-Level BPE，WordPiece，Unigram，SentencePiece

- Byte-Pair Encoding (BPE) Tokenization
    - BPE算法包含两个部分：'词频统计'与‘词表合并’
    1. 词频统计pre-tokenization：找相邻的且出现频率高的pair，把它放入基本词表里面
    2. 词表合并：
    - 缺点：包含所有可能的基本字符token的基本词汇可能相当大；
    - 改进：Byte-level BPE
    1.将字节byte视为基本token，两个字节合并即可以表示unicode字符
    
## chunks cut
### 1. 固定长度切分
- 方法：将文档按固定的字符数、词数或句子数进行切分。
- 适用场景：适用于**内容相对独立、上下文之间关联较弱**的文档，比如社交媒体帖子、新闻文章等。
- 优点：简单直接，易于实现。
- 缺点：可能会导致**语义断裂**，某些重要信息被截断，尤其是当文档的上下文信息较强时。
### 2. 按段落切分

- 方法：根据文档的段落结构进行切分，每个段落作为一个块。
- 适用场景：适用于结构清晰、段落逻辑较强的文档，如**论文、报告、技术文档**等。
- 优点：保持了段落的完整性，能够更好地保留语义和逻辑结构。
- 缺点：如果段落过长，可能需要进一步切分；如果段落过短，可能导致块的长度不均衡。
### 3. 按句子切分
- 方法：将文档按句子进行切分，每个句子或几个句子作为一个块。
- 适用场景：适用于**需要精确理解每个句子含义**的任务，如问答系统、情感分析等。
- 优点：能够保持句子的完整性，避免语义的丢失。
- 缺点：可能导致块的长度不均衡，如果句子之间关联紧密，切分可能会影响上下文理解。
### 4. 滑动窗口切分
- 方法：使用滑动窗口技术，在每次切分时重叠部分内容。例如，每次切分200个单词，但每次滑动100个单词，这样保证每个块之间有100个单词的重叠。
- 适用场景：适用于需要**保留上下文信息**的任务，如文本生成、机器翻译等。
- 优点：能够保留部分上下文信息，减少信息丢失的风险。
- 缺点：会增加计算量和数据冗余。
### 5. 按语义块切分
- 方法：基于语义分析，识别出文档中语义上相关的部分，将其作为一个块。例如，通过分段、主题或语义角色标注等技术来确定切分点。
- 适用场景：适用于需要精确理解语义的任务，如摘要生成、内容分类等。
- 优点：保留了较好的语义连贯性。
- 缺点：**实现起来较复杂**，需要使用自然语言处理技术。
### 6. 基于内容结构的切分
- 方法：利用文档的**内容结构（如标题、子标题、列表项等）进行切分**。
- 适用场景：适用于结构化文档，如技术文档、法律条款、书籍等。
- 优点：保留了文档的逻辑结构，方便进一步处理。
- 缺点：对于没有明确结构的文档不适用。
### 7. 动态切分
- 方法：根据任务需求动态调整切分方式。例如，根据每个块的实际内容和长度对切分进行调整，确保块的内容连贯性和长度适中。
- 适用场景：适用于**复杂的文本分析任务**，特别是当需要在不同文档类型之间切换时。
- 优点：灵活性强，可以根据具体需求调整切分策略。
- 缺点：实现难度较高，可能需要结合多种技术。

如何选择切分方法？
根据任务需求：如果需要保持上下文，选择滑动窗口或语义块切分；如果任务更关注独立的信息提取，固定长度或段落切分可能更合适。
根据文档结构：文档有清晰结构时，利用内容结构切分；否则，可以考虑按句子或段落切分。
根据计算资源：滑动窗口和动态切分会增加计算量，如果资源有限，可以选择较简单的切分方式。

## embedding model

### 1. 文本嵌入模型
- Word2Vec：通过两种方法（Skip-gram 和 Continuous Bag of Words，CBOW）学习词语的向量表示，能够捕捉词语之间的语义关系。
- GloVe（Global Vectors for Word Representation）：基于全局共现矩阵，将词语嵌入到向量空间中，捕捉词语的共现统计信息。
- FastText：扩展了Word2Vec，将词分解为n-grams，因此能更好地处理未登录词（out-of-vocabulary words）。
- BERT（Bidirectional Encoder Representations from Transformers）：基于Transformer架构，能够捕捉上下文语义信息，适合复杂的自然语言处理任务。
- GPT系列（Generative Pre-trained Transformer）：以生成式模型为基础，能够生成上下文相关的文本嵌入，应用于文本生成、对话系统等。
- ELMo（Embeddings from Language Models）：基于双向LSTM，生成上下文感知的词向量，能够处理多义词的问题。
### 2. 句子/段落嵌入模型
- InferSent：基于BiLSTM的句子嵌入模型，通过自然语言推理（NLI）数据集训练，适用于语义相似性任务。
- Universal Sentence Encoder（USE）：谷歌开发的模型，适用于多种语言的句子嵌入，能够捕捉句子的整体语义。
- Sentence-BERT：将BERT用于句子嵌入，专为语义相似性和检索任务设计，速度更快，性能优越。
- Doc2Vec：扩展了Word2Vec，用于生成段落或文档级别的嵌入，适合文档分类、检索等任务。
### 3. 图像嵌入模型
- ResNet：残差网络，常用于图像分类任务，通过提取图像的高维特征，可以生成图像嵌入。
- VGG：视觉几何组网络，另一种常用于图像嵌入的卷积神经网络，结构简单且易于实现。
- Inception：谷歌提出的网络架构，融合了多种卷积核尺寸，适用于图像嵌入任务。
- EfficientNet：一种更高效的卷积神经网络模型，通过复合缩放方法在不同的计算资源下表现良好。
### 4. 图嵌入模型
- Node2Vec：基于Word2Vec的原理，用于图结构数据的节点嵌入，能够捕捉节点的邻居结构信息。
- GraphSAGE：一种扩展图卷积网络（GCN）的模型，能够在大规模图数据上高效生成节点嵌入。
- DeepWalk：通过随机游走生成节点序列，结合Word2Vec生成节点嵌入，适用于无监督图嵌入任务。
### 5. 多模态嵌入模型
- CLIP（Contrastive Language–Image Pretraining）：由OpenAI开发，用于将图像和文本嵌入到同一个向量空间，从而支持跨模态检索。
- ViLT（Vision-and-Language Transformer）：用于将视觉和语言数据联合嵌入，适用于图像文本联合理解任务。
### 6. 其他嵌入模型
- T-SNE：用于将高维数据（如文本或图像）嵌入到低维空间中，以便于可视化。
- UMAP：一种流行的降维技术，能够在保持全局结构的同时，更好地表现局部结构，用于数据的可视化和嵌入。



皮尔逊相关系数（Pearson Correlation Coefficient）：这种方法用于度量两个变量之间的线性相关性，取值范围为[-1, 1]。其中，1表示完全正相关，-1表示完全负相关，0表示不相关。

余弦相似度（Cosine Similarity）：余弦相似度用于度量两个向量之间的夹角余弦值，其取值范围同样为[-1, 1]。其中，1表示完全相似，-1表示完全不相似。

欧氏距离（Euclidean Distance）：欧氏距离用于度量两个向量之间的欧氏距离，即两个向量各个元素差的平方和再开根号。其取值范围为[0, +∞)，其中0表示完全相同。

曼哈顿距离（Manhattan Distance）：曼哈顿距离用于度量两个向量之间的曼哈顿距离，即两个向量各个元素差的绝对值之和。其取值范围同样为[0, +∞)，其中0表示完全相同。
