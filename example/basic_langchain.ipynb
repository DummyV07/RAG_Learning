{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于langchain框架实现基础的RAG流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文件的读取与切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/tensorRT基础.pdf\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 文件格式包括 pdf word excel\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredFileLoader, TextLoader\n",
    "\n",
    "# 读取文件，可以读取网络或者本地文件\n",
    "loader = PyPDFLoader(\"./data/tensorRT基础.pdf\") \n",
    "\n",
    "# 属性：\n",
    "# file_path: 存储PDF文件的路径。\n",
    "print(loader.file_path)\n",
    "# metadata: 可能包含与文档相关的元数据。\n",
    "print(loader.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法：\n",
    "# load(): 读取PDF内容并返回文档对象。\n",
    "# get_metadata(): 获取文档的元数据。\n",
    "pages = loader.load_and_split()\n",
    "# print(loader.load()[1])# 表格识别失败 (直接就不给我识别)\n",
    "# print(loader.load()[2])# 顺序格式识别错误\n",
    "# print(loader.load()[3])# 不处理图片(截图)\n",
    "# print(loader.load()[4])# 部分代码框也没有识别出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分 chunk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='TensorRT 基础  \n",
      "TensorRT 基 础\n",
      "一 、 准 备 知 识\n",
      "1.1 环 境 配 置\n",
      "A. CUDA Driver\n",
      "B. CUDA\n",
      "C. cuDNN\n",
      "D. TensorRT\n",
      "1.2  TensorRT 介 绍\n",
      "二 、 构 建 阶 段\n",
      "2.1 创 建 网 络 定 义\n",
      "2.2 配 置 参 数\n",
      "2.3 生 成 Engine\n",
      "2.4 保 存 为 模 型 文 件\n",
      "2.5 释 放 资 源\n",
      "三 、 运 行 时 阶 段\n",
      "3.1 反 序 列 化 并 创 建 Engine\n",
      "3.2 创 建 一 个ExecutionContext\n",
      "3.3 为 推 理 填 充 输 入\n",
      "3.4 调 用 enqueueV2 来 执 行 推 理\n",
      "3.5 释 放 资 源\n",
      "四 、 编 译 和 运 行\n",
      "一、准备知识  \n",
      "NVIDIA® TensorRT™ 是一个用于高性能深度学习的推理框架。它可以与 TensorFlow 、 PyTorch 和\n",
      "MXNet 等训练框架相辅相成地工作。\n",
      "1.1 环境配置  \n",
      "A. CUDA Driver  \n",
      "使用 CUDA 前，要求 GPU 驱动与cuda 的版本要匹配，匹配关系如下：' metadata={'source': './data/tensorRT基础.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分 chunk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,                           # 切分大小 这两个参数设置的依据是什么\n",
    "    chunk_overlap  = 50,                        # 切分重叠\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/tensorRT基础.pdf', 'page': 7}, page_content='ii  tensorrt \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa08.5.3.1-1+cuda11.8 \\xa0 \\xa0 \\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0  amd64 \\xa0 \\xa0 \\xa0  Meta package for TensorRT')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[20]# 需要对数据进行清洗，有些读取切分出来的完全没有意义 这里有两种优化方式 1 chunk可以根据语意进行切割 2使用大模型对文档内容进行理解精简之后再存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 将文字转成编码\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载.env文件获取API-key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "embed_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据库\n",
    "\n",
    "数据库选用chromdb \n",
    "\n",
    "数据使用data文件夹中的不同格式的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=docs, embedding=embed_model,\n",
    "                                     collection_name=\"my_collection\",persist_directory=\"./directory\") # \n",
    "# 持久化到磁盘\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 向量数据集的读取\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 将文字转成编码\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载.env文件获取API-key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "embed_model = OpenAIEmbeddings()\n",
    "# 加载已保存的向量数据库\n",
    "loaded_vector_store = Chroma(\n",
    "    collection_name=\"my_collection\",\n",
    "    persist_directory=\"./directory\",\n",
    "    embedding_function=embed_model  # 重要：提供嵌入模型\n",
    ")\n",
    "# 追加新的文档\n",
    "# loaded_vector_store.add_documents(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提高了模型的适应性。\n",
      "7. 集成 TensorFlow 和 TensorFlow Lite ： TensorRT 可以与 TensorFlow 和 TensorFlow Lite 集\n",
      "成，使得在 NVIDIA GPU 上执行 TensorFlow 模型变得更为高效。\n",
      "8. 插件支持：  TensorRT 提供了插件支持，可以通过插件自定义层的实现，以适应一些特殊的网\n",
      "络结构。\n",
      "TensorRT 可以在边缘设备、嵌入式系统和数据中心等场景中使用，以加速深度学习模型的推理任\n",
      "务。在需要实时性能和低延迟的应用中， TensorRT 是一个强大的工具，可用于优化和部署深度学\n",
      "习模型。\n",
      " \n",
      "TensorRT 分两个阶段运行\n",
      "构建（Build ）阶段：你向 TensorRT 提供一个模型定义， TensorRT 为目标 GPU 优化这个模型。这\n",
      "个过程可以离线运行。\n",
      "[{'page': 8, 'source': './data/tensorRT基础.pdf'}, {'page': 2, 'source': './data/tensorRT基础.pdf'}, {'page': 12, 'source': './data/tensorRT基础.pdf'}, {'page': 13, 'source': './data/tensorRT基础.pdf'}, {'page': 7, 'source': './data/tensorRT基础.pdf'}, {'page': 6, 'source': './data/tensorRT基础.pdf'}, {'page': 10, 'source': './data/tensorRT基础.pdf'}, {'page': 11, 'source': './data/tensorRT基础.pdf'}, {'page': 7, 'source': './data/tensorRT基础.pdf'}, {'page': 6, 'source': './data/tensorRT基础.pdf'}, {'page': 5, 'source': './data/tensorRT基础.pdf'}, {'page': 7, 'source': './data/tensorRT基础.pdf'}, {'page': 3, 'source': './data/tensorRT基础.pdf'}, {'page': 4, 'source': './data/tensorRT基础.pdf'}, {'page': 10, 'source': './data/tensorRT基础.pdf'}, {'page': 0, 'source': './data/tensorRT基础.pdf'}, {'page': 2, 'source': './data/tensorRT基础.pdf'}, {'page': 11, 'source': './data/tensorRT基础.pdf'}, {'page': 1, 'source': './data/tensorRT基础.pdf'}, {'page': 8, 'source': './data/tensorRT基础.pdf'}, {'page': 7, 'source': './data/tensorRT基础.pdf'}, {'page': 4, 'source': './data/tensorRT基础.pdf'}, {'page': 5, 'source': './data/tensorRT基础.pdf'}, {'page': 14, 'source': './data/tensorRT基础.pdf'}, {'page': 12, 'source': './data/tensorRT基础.pdf'}, {'page': 1, 'source': './data/tensorRT基础.pdf'}, {'page': 12, 'source': './data/tensorRT基础.pdf'}, {'page': 1, 'source': './data/tensorRT基础.pdf'}, {'page': 9, 'source': './data/tensorRT基础.pdf'}, {'page': 2, 'source': './data/tensorRT基础.pdf'}, {'page': 7, 'source': './data/tensorRT基础.pdf'}, {'page': 0, 'source': './data/tensorRT基础.pdf'}, {'page': 11, 'source': './data/tensorRT基础.pdf'}, {'page': 10, 'source': './data/tensorRT基础.pdf'}, {'page': 13, 'source': './data/tensorRT基础.pdf'}, {'page': 13, 'source': './data/tensorRT基础.pdf'}, {'page': 2, 'source': './data/tensorRT基础.pdf'}, {'page': 10, 'source': './data/tensorRT基础.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "# 获取数据库中的部分数据\n",
    "data = loaded_vector_store.get()\n",
    "print(data['documents'][0])  # 打印文档内容\n",
    "print(data['metadatas'])  # 打印元数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT 的网络定义不会复制参数数组（如卷积的权重）。因此，在构建阶段完成之前，你不能释放这\n",
      "些数组的内存。\n",
      "2.2 配置参数  \n",
      "下面我们来添加相关 Builder 的配置。 createBuilderConfig 接口被用来指定 TensorRT 应该如何优\n",
      "化模型。如下：\n",
      "在可用的配置选项中，你可以控制 TensorRT 降低计算精度的能力，控制内存和运行时执行速度之间的权\n",
      "衡，并限制 CUDA® 内核的选择。由于构建器的运行可能需要几分钟或更长时间，你也可以控制构建器如\n",
      "何搜索内核，以及缓存搜索结果以用于后续运行。在我们的示例代码中，我们仅配置\n",
      "workspace （workspace 就是  tensorrt 里面算子可用的内存空间  ）大小和运行时 batch size ，如\n",
      "下：\n",
      "2.3 生成 Engine  \n",
      "在你有了网络定义和 Builder 配置后，你可以调用 Builder 来创建Engine 。Builder 以一种称为\n",
      "plan 的序列化形式创建 Engine ，它可以立即反序列化，也可以保存到磁盘上供以后使用。需要注意的\n",
      "------------------------------\n",
      "构建阶段的最高级别接口是  Builder 。Builder 负责优化一个模型，并产生 Engine 。通过如下接口创\n",
      "建一个Builder 。\n",
      "要生成一个可以进行推理的 Engine ，一般需要以下三个步骤：\n",
      "创建一个网络定义\n",
      "填写Builder 构建配置参数，告诉构建器应该如何优化模型\n",
      "调用Builder 生成Engine\n",
      "2.1 创建网络定义  \n",
      "NetworkDefinition 接口被用来定义模型。如下所示：\n",
      "接口createNetworkV2 接受配置参数，参数用按位标记的方式传入。比如上面激活 explicitBatch ，\n",
      "是通过1U << static_cast<uint32_t>\n",
      "(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); 将explicitBatch 对应的配置位\n",
      "设置为 1 实现的。在新版本中，请使用 createNetworkV2 而非其他任何创建 NetworkDefinition 的接\n",
      "口。\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 查询数据库\n",
    "query = \"添加相关 Builder 的配置。\"\n",
    "results = loaded_vector_store.similarity_search(query,k=2)\n",
    "\n",
    "# 显示结果\n",
    "for result in results:\n",
    "    print(result.page_content)  # 打印文档内容\n",
    "    print('-'*30)\n",
    "    # print(result.metadata)  # 打印元数据\n",
    "\n",
    "\n",
    "# 这里的优化思路是rerank\n",
    "# 重排序（Re-Rank）：对检索出的文档进行重新排序，以确保最相关的文档排在前面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结下来这个数据库用着不舒服 想要有个带ui界面可视化的那种\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/dummy/anaconda3/envs/pytorch/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 构建prompt的时候也有几点优化\n",
    "# 比如query优化\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 将文字转成编码\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 加载.env文件获取API-key\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "embed_model = OpenAIEmbeddings()\n",
    "# 加载已保存的向量数据库\n",
    "loaded_vector_store = Chroma(\n",
    "    collection_name=\"my_collection\",\n",
    "    persist_directory=\"./directory\",\n",
    "    embedding_function=embed_model  # 重要：提供嵌入模型\n",
    ")\n",
    "# 追加新的文档\n",
    "\n",
    "# loaded_vector_store.add_documents(new_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query,loaded_vector_store):\n",
    "    # 从向量数据库中获取top3的文本片段\n",
    "    result = loaded_vector_store.similarity_search(query,k=3)\n",
    "    \n",
    "    source_knowleage = '/n'.join([x.page_content for x in result])\n",
    "    # 构建prompt\n",
    "    augment_prompt = f\"\"\"\n",
    "    使用以下信息，回答问题：\n",
    "    信息：\n",
    "    {source_knowleage}\n",
    "    问题：\n",
    "    {query}\n",
    "    \"\"\"\n",
    "    return augment_prompt\n",
    "prompt = augment_prompt(\"如何添加相关Builder\",loaded_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    使用以下信息，回答问题：\n",
      "    信息：\n",
      "    构建阶段的最高级别接口是  Builder 。Builder 负责优化一个模型，并产生 Engine 。通过如下接口创\n",
      "建一个Builder 。\n",
      "要生成一个可以进行推理的 Engine ，一般需要以下三个步骤：\n",
      "创建一个网络定义\n",
      "填写Builder 构建配置参数，告诉构建器应该如何优化模型\n",
      "调用Builder 生成Engine\n",
      "2.1 创建网络定义  \n",
      "NetworkDefinition 接口被用来定义模型。如下所示：\n",
      "接口createNetworkV2 接受配置参数，参数用按位标记的方式传入。比如上面激活 explicitBatch ，\n",
      "是通过1U << static_cast<uint32_t>\n",
      "(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); 将explicitBatch 对应的配置位\n",
      "设置为 1 实现的。在新版本中，请使用 createNetworkV2 而非其他任何创建 NetworkDefinition 的接\n",
      "口。/nTensorRT 的网络定义不会复制参数数组（如卷积的权重）。因此，在构建阶段完成之前，你不能释放这\n",
      "些数组的内存。\n",
      "2.2 配置参数  \n",
      "下面我们来添加相关 Builder 的配置。 createBuilderConfig 接口被用来指定 TensorRT 应该如何优\n",
      "化模型。如下：\n",
      "在可用的配置选项中，你可以控制 TensorRT 降低计算精度的能力，控制内存和运行时执行速度之间的权\n",
      "衡，并限制 CUDA® 内核的选择。由于构建器的运行可能需要几分钟或更长时间，你也可以控制构建器如\n",
      "何搜索内核，以及缓存搜索结果以用于后续运行。在我们的示例代码中，我们仅配置\n",
      "workspace （workspace 就是  tensorrt 里面算子可用的内存空间  ）大小和运行时 batch size ，如\n",
      "下：\n",
      "2.3 生成 Engine  \n",
      "在你有了网络定义和 Builder 配置后，你可以调用 Builder 来创建Engine 。Builder 以一种称为\n",
      "plan 的序列化形式创建 Engine ，它可以立即反序列化，也可以保存到磁盘上供以后使用。需要注意的/n口。\n",
      "将模型转移到 TensorRT 的最常见的方式是以 ONNX 格式从框架中导出（将在后续课程进行介绍），并使\n",
      "用TensorRT 的 ONNX 解析器来填充网络定义。同时，也可以使用 TensorRT 的Layer 和Tensor 等接口一\n",
      "步一步地进行定义。通过接口来定义网络的代码示例如下：\n",
      "添加输入层\n",
      "添加全连接层\n",
      "添加激活层\n",
      "通过调用 network 的方法，我们可以构建网络的定义。\n",
      "无论你选择哪种方式，你还必须定义哪些张量是网络的输入和输出。没有被标记为输出的张量被认为是\n",
      "瞬时值，可以被构建者优化掉。输入和输出张量必须被命名，以便在运行时， TensorRT 知道如何将输入\n",
      "和输出缓冲区绑定到模型上。示例代码如下：nvinfer1::IBuilder* builder = nvinfer1::createInferBuilder(logger);\n",
      "// bit shift，移位： y 左移 N 位，相当于  y * 2^N\n",
      "// kEXPLICIT_BATCH （显性 Batch ）为 0 ， 1U << 0 = 1\n",
      "// static_cast ：强制类型转换\n",
      "    问题：\n",
      "    如何添加相关Builder\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用部署好的大模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用transform库进行模型加载及推理实验\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = augment_prompt(\"什么是Builder\",loaded_vector_store)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是千问，一个由阿里云开发的大语言模型\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builder 是 TensorFlow 的一个组件，负责优化和创建机器学习模型。它是一个抽象类，包含一系列方法，帮助用户创建和管理复杂的模型架构。\n",
      "\n",
      "以下是 `Builder` 类的一些关键特性：\n",
      "\n",
      "1. **优化功能**：Builder 提供了多种优化策略，包括自动微分、批量归一化、动态重叠等，可以帮助用户快速构建高性能的模型。\n",
      "\n",
      "2. **创建网络定义**：`createNetworkV2` 接口允许开发者定义模型的结构，例如是否启用显式批处理、特定网络的框架选择等。\n",
      "\n",
      "3. **配置参数**：Builder 支持通过配置参数定制模型，包括计算精度、内存管理和运行时性能等方面的设定。\n",
      "\n",
      "4. **计划与反序列化**：Builder 可以创建引擎并立即反序列化，或者将其保存到磁盘以便于后续使用。\n",
      "\n",
      "5. **生命周期管理**：Builder 允许用户根据需要随时清理资源，确保资源的有效利用。\n",
      "\n",
      "6. **支持不同优化技术**：Builder 支持多种优化技术，包括手动微分、批量归一化等，用户可以根据自己的需求选择合适的优化方案。\n",
      "\n",
      "7. **跨平台兼容性**：Builder 旨在提供一个通用的方法来创建和优化模型，适用于各种设备和架构。\n",
      "\n",
      "总之，Builder 是 TensorFlow 中的一个核心工具，提供了强大的优化能力，使得用户能够快速构建高性能的模型。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    使用以下信息，回答问题：\n",
      "    信息：\n",
      "    构建阶段的最高级别接口是  Builder 。Builder 负责优化一个模型，并产生 Engine 。通过如下接口创\n",
      "建一个Builder 。\n",
      "要生成一个可以进行推理的 Engine ，一般需要以下三个步骤：\n",
      "创建一个网络定义\n",
      "填写Builder 构建配置参数，告诉构建器应该如何优化模型\n",
      "调用Builder 生成Engine\n",
      "2.1 创建网络定义  \n",
      "NetworkDefinition 接口被用来定义模型。如下所示：\n",
      "接口createNetworkV2 接受配置参数，参数用按位标记的方式传入。比如上面激活 explicitBatch ，\n",
      "是通过1U << static_cast<uint32_t>\n",
      "(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); 将explicitBatch 对应的配置位\n",
      "设置为 1 实现的。在新版本中，请使用 createNetworkV2 而非其他任何创建 NetworkDefinition 的接\n",
      "口。/nTensorRT 的网络定义不会复制参数数组（如卷积的权重）。因此，在构建阶段完成之前，你不能释放这\n",
      "些数组的内存。\n",
      "2.2 配置参数  \n",
      "下面我们来添加相关 Builder 的配置。 createBuilderConfig 接口被用来指定 TensorRT 应该如何优\n",
      "化模型。如下：\n",
      "在可用的配置选项中，你可以控制 TensorRT 降低计算精度的能力，控制内存和运行时执行速度之间的权\n",
      "衡，并限制 CUDA® 内核的选择。由于构建器的运行可能需要几分钟或更长时间，你也可以控制构建器如\n",
      "何搜索内核，以及缓存搜索结果以用于后续运行。在我们的示例代码中，我们仅配置\n",
      "workspace （workspace 就是  tensorrt 里面算子可用的内存空间  ）大小和运行时 batch size ，如\n",
      "下：\n",
      "2.3 生成 Engine  \n",
      "在你有了网络定义和 Builder 配置后，你可以调用 Builder 来创建Engine 。Builder 以一种称为\n",
      "plan 的序列化形式创建 Engine ，它可以立即反序列化，也可以保存到磁盘上供以后使用。需要注意的/n口。\n",
      "将模型转移到 TensorRT 的最常见的方式是以 ONNX 格式从框架中导出（将在后续课程进行介绍），并使\n",
      "用TensorRT 的 ONNX 解析器来填充网络定义。同时，也可以使用 TensorRT 的Layer 和Tensor 等接口一\n",
      "步一步地进行定义。通过接口来定义网络的代码示例如下：\n",
      "添加输入层\n",
      "添加全连接层\n",
      "添加激活层\n",
      "通过调用 network 的方法，我们可以构建网络的定义。\n",
      "无论你选择哪种方式，你还必须定义哪些张量是网络的输入和输出。没有被标记为输出的张量被认为是\n",
      "瞬时值，可以被构建者优化掉。输入和输出张量必须被命名，以便在运行时， TensorRT 知道如何将输入\n",
      "和输出缓冲区绑定到模型上。示例代码如下：nvinfer1::IBuilder* builder = nvinfer1::createInferBuilder(logger);\n",
      "// bit shift，移位： y 左移 N 位，相当于  y * 2^N\n",
      "// kEXPLICIT_BATCH （显性 Batch ）为 0 ， 1U << 0 = 1\n",
      "// static_cast ：强制类型转换\n",
      "    问题：\n",
      "    如何添加相关Builder\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试结果其实并不是很好 一方面是因为query和context的关系并不大 所以我们是否要考虑不使用 向量数据库 尝试使用知识图谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
